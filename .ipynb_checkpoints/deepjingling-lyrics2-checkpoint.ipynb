{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from cntk.blocks import default_options, LSTM, Placeholder, Input\n",
    "from cntk.layers import Embedding, Recurrence, Dense, BatchNormalization\n",
    "from cntk.models import Sequential\n",
    "from cntk.utils import ProgressPrinter, log_number_of_parameters\n",
    "from cntk.io import MinibatchSource, CTFDeserializer, StreamDef, StreamDefs, INFINITELY_REPEAT, FULL_DATA_SWEEP\n",
    "from cntk import *\n",
    "from cntk.learner import sgd, adam_sgd, learning_rate_schedule\n",
    "from cntk.device import set_default_device, gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set to GPU, run if GPU is available\n",
    "set_default_device(gpu(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set random seed (don't know if this actually works for reproducing results)\n",
    "random.seed(1)\n",
    "\n",
    "# Set paths\n",
    "raw_data_path = 'data/songs.txt'\n",
    "data_path = 'data/songs_processed-2.ctf'\n",
    "dict_path = 'data/dict-2.ctf'\n",
    "\n",
    "# Read text file and convert to lower case\n",
    "with open(raw_data_path, encoding='utf8') as f:\n",
    "    source_text = f.read().lower()\n",
    "\n",
    "# Define and make char replacements\n",
    "replacements = [[\"’\", \"'\"], \n",
    "                ['“', '\"'], \n",
    "                ['”', '\"'],\n",
    "                ['\\n', '$']]\n",
    "for r in replacements:\n",
    "    source_text = source_text.replace(r[0], r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30647"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get length of source text - it is quite small for an RNN!\n",
    "len(source_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary of characters\n",
    "chars = [[k, v] for v, k in enumerate(sorted(set(source_text)))]\n",
    "char_dict = {key: value for (key, value) in chars}\n",
    "\n",
    "# Get number of songs (the beginning and end of songs are marked with '|')\n",
    "nb_songs = source_text.count('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set max length of sequences - 10 should be enough to learn the network how to spell words\n",
    "seq_max_length = 10\n",
    "\n",
    "# Iterate through source text and create appropriate sequence format for CNTK\n",
    "new_text = ''\n",
    "nb_sequences = 0\n",
    "for n, char in enumerate(source_text[1:]):\n",
    "    prev_chars = source_text[max(0,(n+1-seq_max_length)):n+1]\n",
    "    if '|' in prev_chars:\n",
    "        prev_chars = prev_chars[max(0,prev_chars.index('|')):]\n",
    "    for k, prev_char in enumerate(prev_chars):\n",
    "        new_text += str(n) + '\\t|ic ' + str(char_dict[prev_char]) + ':1'\n",
    "        if k == 0:\n",
    "            new_text += '\\t|oc ' + str(char_dict[char]) + ':1'\n",
    "        new_text += '\\n'\n",
    "        nb_sequences += 1\n",
    "\n",
    "# Write string to file\n",
    "with open(data_path, \"w\") as text_file:\n",
    "    text_file.write(new_text)\n",
    "        \n",
    "# Create dictionary string\n",
    "dict_text = ''\n",
    "for l in sorted(char_dict, key=char_dict.get):\n",
    "    dict_text += l + '\\n'\n",
    "\n",
    "# Write dictionary to file\n",
    "with open(dict_path, \"w\") as dict_file:\n",
    "    dict_file.write(dict_text)\n",
    "\n",
    "# Get number of sequences\n",
    "nb_sequences = len(source_text) - 1\n",
    "\n",
    "# Number of chars in vocabulary\n",
    "vocab_size = num_labels = len(char_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model dimensions\n",
    "input_dim = vocab_size\n",
    "label_dim = num_labels\n",
    "hidden_dim = 256\n",
    "\n",
    "# Function to create model\n",
    "def create_model():\n",
    "    with default_options(initial_state=0.1):\n",
    "        # Batch normalization seems to help stabilize the initial learning, but doesn't work on CPU at the moment\n",
    "        return Sequential([\n",
    "                Recurrence(LSTM(hidden_dim), go_backwards=False), \n",
    "                BatchNormalization(),\n",
    "                Dense(num_labels)\n",
    "            ])\n",
    "\n",
    "def create_reader(path, is_training):\n",
    "    ic_stream = StreamDef(field='ic', shape=vocab_size, is_sparse=True)\n",
    "    oc_stream = StreamDef(field='oc', shape=num_labels, is_sparse=True)\n",
    "    stream_defs = StreamDefs(ic = ic_stream, oc = oc_stream)\n",
    "    ctf_deserializer = CTFDeserializer(path, stream_defs)\n",
    "    mbs = MinibatchSource(ctf_deserializer, randomize=is_training, \n",
    "                          epoch_size = INFINITELY_REPEAT if is_training else FULL_DATA_SWEEP)\n",
    "    return mbs\n",
    "\n",
    "def create_criterion_function(model):\n",
    "    labels = Placeholder()\n",
    "    ce = cross_entropy_with_softmax(model, labels)\n",
    "    errs = classification_error(model, labels)\n",
    "    return combine ([ce, errs])\n",
    "\n",
    "def train(reader, model, max_epochs=1000):\n",
    "    criterion = create_criterion_function(model)\n",
    "    criterion.replace_placeholders({criterion.placeholders[0]: Input(vocab_size), \n",
    "                                    criterion.placeholders[1]: Input(num_labels)})\n",
    "    \n",
    "    # Set epoch size; usually one pass of the data set, but CNTK doesn't really care about this\n",
    "    epoch_size = 100000\n",
    "    \n",
    "    # Set minibatch size - is this really sequences, or is it samples?\n",
    "    minibatch_size = 100\n",
    "    \n",
    "    # Set learning rate schedule - a flat 0.001 usually works very well for Adam, since it should\n",
    "    # adaptively decay the learning rate for each parameter. However, CNTK does not seem to agree ...\n",
    "    #lr_schedule = learning_rate_schedule([(15, 0.1), (15, 0.01), (15, 0.001), (1, 0.0001)], UnitType.sample, epoch_size)\n",
    "    lr_schedule = learning_rate_schedule(0.001, UnitType.sample)\n",
    "    \n",
    "    # Set momentum schedule\n",
    "    #momentum_as_time_constant = momentum_as_time_constant_schedule(700)\n",
    "    m_schedule = momentum_schedule(0.95)\n",
    "    \n",
    "    # Define optimizer\n",
    "    #learner = sgd(criterion.parameters, lr=lr_schedule)\n",
    "    learner = adam_sgd(criterion.parameters, lr=lr_schedule, momentum=m_schedule)\n",
    "    \n",
    "    # Define trainer\n",
    "    trainer = Trainer(model, criterion.outputs[0], criterion.outputs[1], learner)\n",
    "    \n",
    "    # Process minibatches and perform training\n",
    "    log_number_of_parameters(model)\n",
    "    progress_printer = ProgressPrinter(freq=1000, tag='Training')\n",
    "    \n",
    "    t = 0\n",
    "    n = 0\n",
    "    for epoch in range(max_epochs):\n",
    "        epoch_end = (epoch+1) * epoch_size\n",
    "        while t < epoch_end:\n",
    "            if n == 0:\n",
    "                mask = True\n",
    "            else:\n",
    "                mask = False\n",
    "            n += 1\n",
    "            data = reader.next_minibatch(minibatch_size, input_map={\n",
    "                    criterion.arguments[0]: reader.streams.ic, \n",
    "                    criterion.arguments[1]: reader.streams.oc\n",
    "                })\n",
    "            trainer.train_minibatch((data, [False]*data[criterion.arguments[1]].num_samples))\n",
    "            t += data[criterion.arguments[1]].num_samples\n",
    "            progress_printer.update_with_trainer(trainer, with_metric=True)\n",
    "        loss, metric, actual_samples = progress_printer.epoch_summary(with_metric=True)\n",
    "    \n",
    "    return loss, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 325168 parameters in 7 parameter tensors.\n",
      " Minibatch[   1-1000]: loss = 3.486331 * 10010, metric = 90.2% * 10010\n",
      " Minibatch[1001-2000]: loss = 3.180495 * 10002, metric = 86.6% * 10002\n",
      " Minibatch[2001-3000]: loss = 3.111794 * 10005, metric = 85.7% * 10005\n",
      " Minibatch[3001-4000]: loss = 3.059751 * 10002, metric = 84.3% * 10002\n",
      " Minibatch[4001-5000]: loss = 3.045321 * 10005, metric = 84.2% * 10005\n",
      " Minibatch[5001-6000]: loss = 3.055200 * 10002, metric = 84.5% * 10002\n",
      " Minibatch[6001-7000]: loss = 3.031297 * 9996, metric = 83.8% * 9996\n",
      " Minibatch[7001-8000]: loss = 3.033430 * 10008, metric = 83.8% * 10008\n",
      " Minibatch[8001-9000]: loss = 3.038923 * 10002, metric = 84.4% * 10002\n",
      "Finished Epoch [1]: [Training] loss = 3.107703 * 100002, metric = 85.2% * 100002 19.262s (5191.8 samples per second)\n",
      " Minibatch[   1-1000]: loss = 3.034003 * 19972, metric = 84.2% * 19972\n",
      " Minibatch[1001-2000]: loss = 3.031085 * 10003, metric = 83.9% * 10003\n",
      " Minibatch[2001-3000]: loss = 3.024816 * 10000, metric = 84.0% * 10000\n",
      " Minibatch[3001-4000]: loss = 3.019485 * 10002, metric = 83.8% * 10002\n",
      " Minibatch[4001-5000]: loss = 3.030746 * 10003, metric = 83.8% * 10003\n",
      " Minibatch[5001-6000]: loss = 3.009730 * 10002, metric = 83.3% * 10002\n",
      " Minibatch[6001-7000]: loss = 3.021878 * 10002, metric = 84.1% * 10002\n",
      " Minibatch[7001-8000]: loss = 3.031822 * 10008, metric = 84.1% * 10008\n",
      " Minibatch[8001-9000]: loss = 3.032256 * 9994, metric = 83.9% * 9994\n",
      "Finished Epoch [2]: [Training] loss = 3.025837 * 100000, metric = 83.9% * 100000 19.296s (5182.4 samples per second)\n",
      " Minibatch[   1-1000]: loss = 3.021106 * 19987, metric = 84.1% * 19987\n",
      " Minibatch[1001-2000]: loss = 3.023882 * 9999, metric = 83.3% * 9999\n",
      " Minibatch[2001-3000]: loss = 3.019707 * 10001, metric = 84.1% * 10001\n",
      " Minibatch[3001-4000]: loss = 3.019316 * 10002, metric = 84.2% * 10002\n",
      " Minibatch[4001-5000]: loss = 3.020907 * 10001, metric = 84.1% * 10001\n",
      " Minibatch[5001-6000]: loss = 3.023414 * 10005, metric = 83.6% * 10005\n",
      " Minibatch[6001-7000]: loss = 3.014670 * 10003, metric = 83.6% * 10003\n",
      " Minibatch[7001-8000]: loss = 3.025078 * 10001, metric = 84.4% * 10001\n",
      " Minibatch[8001-9000]: loss = 3.007778 * 10003, metric = 83.9% * 10003\n",
      "Finished Epoch [3]: [Training] loss = 3.019415 * 100000, metric = 83.9% * 100000 19.466s (5137.3 samples per second)\n",
      " Minibatch[   1-1000]: loss = 3.017614 * 19981, metric = 83.5% * 19981\n",
      " Minibatch[1001-2000]: loss = 3.030507 * 10006, metric = 84.3% * 10006\n",
      " Minibatch[2001-3000]: loss = 3.013820 * 10009, metric = 84.0% * 10009\n",
      " Minibatch[3001-4000]: loss = 3.008321 * 10000, metric = 83.5% * 10000\n",
      " Minibatch[4001-5000]: loss = 3.024972 * 10003, metric = 84.3% * 10003\n",
      " Minibatch[5001-6000]: loss = 3.024400 * 10003, metric = 83.7% * 10003\n",
      " Minibatch[6001-7000]: loss = 3.012589 * 10001, metric = 83.8% * 10001\n",
      " Minibatch[7001-8000]: loss = 3.014065 * 10002, metric = 83.8% * 10002\n",
      " Minibatch[8001-9000]: loss = 3.015868 * 10003, metric = 84.0% * 10003\n",
      "Finished Epoch [4]: [Training] loss = 3.017737 * 100006, metric = 83.8% * 100006 19.265s (5191.1 samples per second)\n",
      " Minibatch[   1-1000]: loss = 3.012793 * 19982, metric = 83.6% * 19982\n",
      " Minibatch[1001-2000]: loss = 3.036268 * 10004, metric = 84.2% * 10004\n",
      " Minibatch[2001-3000]: loss = 3.011464 * 10002, metric = 83.9% * 10002\n",
      " Minibatch[3001-4000]: loss = 3.016116 * 10003, metric = 84.0% * 10003\n",
      " Minibatch[4001-5000]: loss = 3.011079 * 10002, metric = 83.5% * 10002\n",
      " Minibatch[5001-6000]: loss = 3.021353 * 9999, metric = 83.8% * 9999\n",
      " Minibatch[6001-7000]: loss = 3.008478 * 10002, metric = 83.8% * 10002\n",
      " Minibatch[7001-8000]: loss = 3.009418 * 10003, metric = 83.6% * 10003\n",
      " Minibatch[8001-9000]: loss = 3.031056 * 10002, metric = 83.9% * 10002\n",
      "Finished Epoch [5]: [Training] loss = 3.017546 * 99992, metric = 83.9% * 99992 19.250s (5194.4 samples per second)\n",
      " Minibatch[   1-1000]: loss = 3.018080 * 19976, metric = 84.0% * 19976\n",
      " Minibatch[1001-2000]: loss = 3.005228 * 10002, metric = 83.4% * 10002\n",
      " Minibatch[2001-3000]: loss = 3.014053 * 10002, metric = 83.7% * 10002\n",
      " Minibatch[3001-4000]: loss = 3.023398 * 10001, metric = 83.7% * 10001\n",
      " Minibatch[4001-5000]: loss = 3.017947 * 10002, metric = 84.1% * 10002\n",
      " Minibatch[5001-6000]: loss = 3.002946 * 9998, metric = 83.5% * 9998\n",
      " Minibatch[6001-7000]: loss = 3.014820 * 10002, metric = 83.6% * 10002\n",
      " Minibatch[7001-8000]: loss = 3.012783 * 10004, metric = 84.0% * 10004\n",
      " Minibatch[8001-9000]: loss = 3.017626 * 10003, metric = 83.6% * 10003\n",
      "Finished Epoch [6]: [Training] loss = 3.013874 * 100008, metric = 83.7% * 100008 19.228s (5201.1 samples per second)\n",
      " Minibatch[   1-1000]: loss = 3.012635 * 19996, metric = 83.9% * 19996\n",
      " Minibatch[1001-2000]: loss = 3.031407 * 9998, metric = 83.8% * 9998\n",
      " Minibatch[2001-3000]: loss = 3.013599 * 10005, metric = 84.0% * 10005\n",
      " Minibatch[3001-4000]: loss = 3.014047 * 10005, metric = 83.8% * 10005\n",
      " Minibatch[4001-5000]: loss = 3.005733 * 9995, metric = 83.9% * 9995\n",
      " Minibatch[5001-6000]: loss = 3.029874 * 10001, metric = 84.3% * 10001\n",
      " Minibatch[6001-7000]: loss = 2.999654 * 10003, metric = 83.2% * 10003\n",
      " Minibatch[7001-8000]: loss = 3.011540 * 10006, metric = 83.5% * 10006\n",
      " Minibatch[8001-9000]: loss = 3.019639 * 10005, metric = 84.0% * 10005\n",
      "Finished Epoch [7]: [Training] loss = 3.014634 * 99996, metric = 83.8% * 99996 19.245s (5195.9 samples per second)\n",
      " Minibatch[   1-1000]: loss = 3.010594 * 19969, metric = 83.9% * 19969\n",
      " Minibatch[1001-2000]: loss = 3.016115 * 10003, metric = 83.6% * 10003\n",
      " Minibatch[2001-3000]: loss = 3.011609 * 10006, metric = 83.7% * 10006\n",
      " Minibatch[3001-4000]: loss = 3.027423 * 9998, metric = 83.9% * 9998\n",
      " Minibatch[4001-5000]: loss = 3.022122 * 10003, metric = 84.0% * 10003\n",
      " Minibatch[5001-6000]: loss = 2.999947 * 10002, metric = 83.3% * 10002\n",
      " Minibatch[6001-7000]: loss = 3.012552 * 10000, metric = 84.5% * 10000\n",
      " Minibatch[7001-8000]: loss = 3.007863 * 10005, metric = 83.4% * 10005\n",
      " Minibatch[8001-9000]: loss = 3.024531 * 10003, metric = 84.0% * 10003\n",
      "Finished Epoch [8]: [Training] loss = 3.013268 * 100004, metric = 83.8% * 100004 19.290s (5184.1 samples per second)\n",
      " Minibatch[   1-1000]: loss = 3.006711 * 19991, metric = 83.7% * 19991\n",
      " Minibatch[1001-2000]: loss = 3.009884 * 10004, metric = 83.5% * 10004\n",
      " Minibatch[2001-3000]: loss = 3.019682 * 9997, metric = 83.9% * 9997\n",
      " Minibatch[3001-4000]: loss = 3.002794 * 10003, metric = 83.4% * 10003\n",
      " Minibatch[4001-5000]: loss = 3.019621 * 10003, metric = 84.0% * 10003\n",
      " Minibatch[5001-6000]: loss = 3.012661 * 10000, metric = 83.6% * 10000\n",
      " Minibatch[6001-7000]: loss = 3.006139 * 10003, metric = 83.7% * 10003\n",
      " Minibatch[7001-8000]: loss = 3.014238 * 10008, metric = 83.6% * 10008\n",
      " Minibatch[8001-9000]: loss = 3.017988 * 9995, metric = 84.0% * 9995\n",
      "Finished Epoch [9]: [Training] loss = 3.014052 * 99996, metric = 83.8% * 99996 19.318s (5176.4 samples per second)\n",
      " Minibatch[   1-1000]: loss = 3.018765 * 19986, metric = 83.8% * 19986\n",
      " Minibatch[1001-2000]: loss = 3.004021 * 10001, metric = 83.6% * 10001\n",
      " Minibatch[2001-3000]: loss = 3.006613 * 10002, metric = 83.4% * 10002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f98478cd58fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdo_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-f98478cd58fa>\u001b[0m in \u001b[0;36mdo_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdo_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-027938b21ac4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(reader, model, max_epochs)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 })\n\u001b[1;32m     76\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mprogress_printer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_with_trainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactual_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprogress_printer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def do_train():\n",
    "    global model\n",
    "    model = create_model()\n",
    "    reader = create_reader(data_path, is_training=True)\n",
    "    train(reader, model)\n",
    "do_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "reader = create_reader(data_path, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion = create_criterion_function(model)\n",
    "criterion.replace_placeholders({criterion.placeholders[0]: Input(vocab_size), \n",
    "                                criterion.placeholders[1]: Input(num_labels)})\n",
    "\n",
    "# Set epoch size; usually one pass of the data set, but CNTK doesn't really care about this\n",
    "epoch_size = 100000\n",
    "\n",
    "# Set minibatch size - is this really sequences, or is it samples?\n",
    "minibatch_size = 100\n",
    "\n",
    "# Set learning rate schedule - a flat 0.001 usually works very well for Adam, since it should\n",
    "# adaptively decay the learning rate for each parameter. However, CNTK does not seem to agree ...\n",
    "#lr_schedule = learning_rate_schedule([(15, 0.1), (15, 0.01), (15, 0.001), (1, 0.0001)], UnitType.sample, epoch_size)\n",
    "lr_schedule = learning_rate_schedule(0.001, UnitType.sample)\n",
    "\n",
    "# Set momentum schedule\n",
    "#momentum_as_time_constant = momentum_as_time_constant_schedule(700)\n",
    "m_schedule = momentum_schedule(0.95)\n",
    "\n",
    "# Define optimizer\n",
    "#learner = sgd(criterion.parameters, lr=lr_schedule)\n",
    "learner = adam_sgd(criterion.parameters, lr=lr_schedule, momentum=m_schedule)\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(model, criterion.outputs[0], criterion.outputs[1], learner)\n",
    "\n",
    "# Process minibatches and perform training\n",
    "log_number_of_parameters(model)\n",
    "progress_printer = ProgressPrinter(freq=1000, tag='Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = reader.next_minibatch(minibatch_size, input_map={\n",
    "                    criterion.arguments[0]: reader.streams.ic, \n",
    "                    criterion.arguments[1]: reader.streams.oc\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[criterion.arguments[1]].num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
