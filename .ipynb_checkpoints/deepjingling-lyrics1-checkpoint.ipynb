{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from cntk.blocks import default_options, LSTM, Placeholder, Input\n",
    "from cntk.layers import Embedding, Recurrence, Dense, BatchNormalization\n",
    "from cntk.models import Sequential\n",
    "from cntk.utils import ProgressPrinter, log_number_of_parameters\n",
    "from cntk.io import MinibatchSource, CTFDeserializer, StreamDef, StreamDefs, INFINITELY_REPEAT, FULL_DATA_SWEEP\n",
    "from cntk import *\n",
    "from cntk.learner import sgd, adam_sgd, learning_rate_schedule\n",
    "from cntk.device import set_default_device, gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set to GPU, run if GPU is available\n",
    "set_default_device(gpu(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set random seed (don't know if this actually works for reproducing results)\n",
    "random.seed(1)\n",
    "\n",
    "# Set paths\n",
    "raw_data_path = 'data/songs.txt'\n",
    "data_path = 'data/songs_processed.ctf'\n",
    "dict_path = 'data/dict.ctf'\n",
    "\n",
    "# Read text file and convert to lower case\n",
    "with open(raw_data_path, encoding='utf8') as f:\n",
    "    source_text = f.read().lower()\n",
    "\n",
    "# Define and make char replacements\n",
    "replacements = [[\"’\", \"'\"], \n",
    "                ['“', '\"'], \n",
    "                ['”', '\"'],\n",
    "                ['\\n', '$']]\n",
    "for r in replacements:\n",
    "    source_text = source_text.replace(r[0], r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get length of source text - it is quite small for an RNN!\n",
    "len(source_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary of characters\n",
    "chars = [[k, v] for v, k in enumerate(sorted(set(source_text)))]\n",
    "char_dict = {key: value for (key, value) in chars}\n",
    "\n",
    "# Get number of songs (the beginning and end of songs are marked with '|')\n",
    "nb_songs = source_text.count('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set max length of sequences - 10 should be enough to learn the network how to spell words\n",
    "seq_max_length = 10\n",
    "\n",
    "# Iterate through source text and create appropriate sequence format for CNTK\n",
    "new_text = ''\n",
    "nb_sequences = 0\n",
    "for n, char in enumerate(source_text[1:]):\n",
    "    prev_chars = source_text[max(0,(n+1-seq_max_length)):n+1]\n",
    "    if '|' in prev_chars:\n",
    "        prev_chars = prev_chars[max(0,prev_chars.index('|')):]\n",
    "    for k, prev_char in enumerate(prev_chars):\n",
    "        new_text += str(n) + '\\t|ic ' + str(char_dict[prev_char]) + ':1'\n",
    "        if k == 0:\n",
    "            new_text += '\\t|oc ' + str(char_dict[char]) + ':1'\n",
    "        new_text += '\\n'\n",
    "        nb_sequences += 1\n",
    "\n",
    "# Write string to file\n",
    "with open(data_path, \"w\") as text_file:\n",
    "    text_file.write(new_text)\n",
    "        \n",
    "# Create dictionary string\n",
    "dict_text = ''\n",
    "for l in sorted(char_dict, key=char_dict.get):\n",
    "    dict_text += l + '\\n'\n",
    "\n",
    "# Write dictionary to file\n",
    "with open(dict_path, \"w\") as dict_file:\n",
    "    dict_file.write(dict_text)\n",
    "\n",
    "# Get number of sequences\n",
    "nb_sequences = len(source_text) - 1\n",
    "\n",
    "# Number of chars in vocabulary\n",
    "vocab_size = num_labels = len(char_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model dimensions\n",
    "input_dim = vocab_size\n",
    "label_dim = num_labels\n",
    "hidden_dim = 256\n",
    "\n",
    "# Function to create model\n",
    "def create_model():\n",
    "    with default_options(initial_state=0.1):\n",
    "        # Batch normalization seems to help stabilize the initial learning, but doesn't work on CPU at the moment\n",
    "        return Sequential([\n",
    "                Recurrence(LSTM(hidden_dim), go_backwards=False), \n",
    "                #BatchNormalization(),\n",
    "                Dense(num_labels)\n",
    "            ])\n",
    "\n",
    "def create_reader(path, is_training):\n",
    "    ic_stream = StreamDef(field='ic', shape=vocab_size, is_sparse=True)\n",
    "    oc_stream = StreamDef(field='oc', shape=num_labels, is_sparse=True)\n",
    "    stream_defs = StreamDefs(ic = ic_stream, oc = oc_stream)\n",
    "    ctf_deserializer = CTFDeserializer(path, stream_defs)\n",
    "    mbs = MinibatchSource(ctf_deserializer, randomize=is_training, \n",
    "                          epoch_size = INFINITELY_REPEAT if is_training else FULL_DATA_SWEEP)\n",
    "    return mbs\n",
    "\n",
    "def create_criterion_function(model):\n",
    "    labels = Placeholder()\n",
    "    ce = cross_entropy_with_softmax(model, labels)\n",
    "    errs = classification_error(model, labels)\n",
    "    return combine ([ce, errs])\n",
    "\n",
    "def train(reader, model, max_epochs=1000):\n",
    "    criterion = create_criterion_function(model)\n",
    "    criterion.replace_placeholders({criterion.placeholders[0]: Input(vocab_size), \n",
    "                                    criterion.placeholders[1]: Input(num_labels)})\n",
    "    \n",
    "    # Set epoch size; usually one pass of the data set, but CNTK doesn't really care about this\n",
    "    epoch_size = 100000\n",
    "    \n",
    "    # Set minibatch size - is this really sequences, or is it samples?\n",
    "    minibatch_size = 100\n",
    "    \n",
    "    # Set learning rate schedule - a flat 0.001 usually works very well for Adam, since it should\n",
    "    # adaptively decay the learning rate for each parameter. However, CNTK does not seem to agree ...\n",
    "    #lr_schedule = learning_rate_schedule([(15, 0.1), (15, 0.01), (15, 0.001), (1, 0.0001)], UnitType.sample, epoch_size)\n",
    "    lr_schedule = learning_rate_schedule(0.001, UnitType.sample)\n",
    "    \n",
    "    # Set momentum schedule\n",
    "    #momentum_as_time_constant = momentum_as_time_constant_schedule(700)\n",
    "    m_schedule = momentum_schedule(0.95)\n",
    "    \n",
    "    # Define optimizer\n",
    "    #learner = sgd(criterion.parameters, lr=lr_schedule)\n",
    "    learner = adam_sgd(criterion.parameters, lr=lr_schedule, momentum=m_schedule)\n",
    "    \n",
    "    # Define trainer\n",
    "    trainer = Trainer(model, criterion.outputs[0], criterion.outputs[1], learner)\n",
    "    \n",
    "    # Process minibatches and perform training\n",
    "    log_number_of_parameters(model)\n",
    "    progress_printer = ProgressPrinter(freq=1000, tag='Training')\n",
    "    \n",
    "    t = 0\n",
    "    for epoch in range(max_epochs):\n",
    "        epoch_end = (epoch+1) * epoch_size\n",
    "        while t < epoch_end:\n",
    "            data = reader.next_minibatch(minibatch_size, input_map={\n",
    "                    criterion.arguments[0]: reader.streams.ic, \n",
    "                    criterion.arguments[1]: reader.streams.oc\n",
    "                })\n",
    "            trainer.train_minibatch(data)\n",
    "            t += data[criterion.arguments[1]].num_samples\n",
    "            progress_printer.update_with_trainer(trainer, with_metric=True)\n",
    "        loss, metric, actual_samples = progress_printer.epoch_summary(with_metric=True)\n",
    "    \n",
    "    return loss, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def do_train():\n",
    "    global model\n",
    "    model = create_model()\n",
    "    reader = create_reader(data_path, is_training=True)\n",
    "    train(reader, model)\n",
    "do_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
