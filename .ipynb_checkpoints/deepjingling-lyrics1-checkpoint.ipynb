{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from cntk.blocks import default_options, LSTM, Placeholder, Input\n",
    "from cntk.layers import Embedding, Recurrence, Dense, BatchNormalization\n",
    "from cntk.models import Sequential\n",
    "from cntk.utils import ProgressPrinter, log_number_of_parameters\n",
    "from cntk.io import MinibatchSource, CTFDeserializer\n",
    "from cntk.io import StreamDef, StreamDefs\n",
    "from cntk.io import INFINITELY_REPEAT, FULL_DATA_SWEEP\n",
    "from cntk import *\n",
    "from cntk.learner import sgd, adam_sgd, learning_rate_schedule\n",
    "from cntk.device import set_default_device, gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_default_device(gpu(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "raw_data_path = 'data/songs.txt'\n",
    "data_path = 'data/songs_processed.ctf'\n",
    "dict_path = 'data/dict.ctf'\n",
    "\n",
    "with open(raw_data_path, encoding='utf8') as f:\n",
    "    source_text = f.read().lower()\n",
    "    \n",
    "replacements = [[\"’\", \"'\"], \n",
    "                ['“', '\"'], \n",
    "                ['”', '\"'],\n",
    "                ['\\n', '$']]\n",
    "\n",
    "# Make replacements\n",
    "for r in replacements:\n",
    "    source_text = source_text.replace(r[0], r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = [[k, v] for v, k in enumerate(sorted(set(source_text)))]\n",
    "char_dict = {key: value for (key, value) in chars}\n",
    "\n",
    "nb_songs = source_text.count('|')\n",
    "seq_max_length = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_text = ''\n",
    "nb_sequences = 0\n",
    "for n, char in enumerate(source_text[1:]):\n",
    "    #if n > 531:\n",
    "    #    break\n",
    "    prev_chars = source_text[max(0,(n+1-seq_max_length)):n+1]\n",
    "    if '|' in prev_chars:\n",
    "        prev_chars = prev_chars[max(0,prev_chars.index('|')):]\n",
    "    for k, prev_char in enumerate(prev_chars):\n",
    "        new_text += str(n) + '\\t|ic ' + str(char_dict[prev_char]) + ':1'\n",
    "        if k == 0:\n",
    "            new_text += '\\t|oc ' + str(char_dict[char]) + ':1'\n",
    "        new_text += '\\n'\n",
    "        nb_sequences += 1\n",
    "\n",
    "dict_text = ''\n",
    "for l in sorted(char_dict, key=char_dict.get):\n",
    "    dict_text += l + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(data_path, \"w\") as text_file:\n",
    "    text_file.write(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(dict_path, \"w\") as dict_file:\n",
    "    dict_file.write(dict_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30646"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_sequences = len(source_text) - 1\n",
    "#nb_sequences = 532\n",
    "nb_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of chars in vocabulary\n",
    "vocab_size = num_labels = len(char_dict)\n",
    "\n",
    "# Model dimensions\n",
    "input_dim = vocab_size\n",
    "label_dim = num_labels\n",
    "hidden_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to create model\n",
    "def create_model():\n",
    "    with default_options(initial_state=0.1):\n",
    "        return Sequential([\n",
    "                Recurrence(LSTM(hidden_dim), go_backwards=False), \n",
    "                BatchNormalization(),\n",
    "                Dense(num_labels)\n",
    "            ])\n",
    "\n",
    "def create_reader(path, is_training):\n",
    "    ic_stream = StreamDef(field='ic', shape=vocab_size, is_sparse=True)\n",
    "    oc_stream = StreamDef(field='oc', shape=num_labels, is_sparse=True)\n",
    "    stream_defs = StreamDefs(ic = ic_stream, oc = oc_stream)\n",
    "    ctf_deserializer = CTFDeserializer(path, stream_defs)\n",
    "    mbs = MinibatchSource(ctf_deserializer, randomize=is_training, \n",
    "                          epoch_size = INFINITELY_REPEAT if is_training else FULL_DATA_SWEEP)\n",
    "    return mbs\n",
    "\n",
    "def create_criterion_function(model):\n",
    "    labels = Placeholder()\n",
    "    ce = cross_entropy_with_softmax(model, labels)\n",
    "    errs = classification_error(model, labels)\n",
    "    return combine ([ce, errs])\n",
    "\n",
    "def train(reader, model, max_epochs=1000):\n",
    "    criterion = create_criterion_function(model)\n",
    "    criterion.replace_placeholders({criterion.placeholders[0]: Input(vocab_size), \n",
    "                                    criterion.placeholders[1]: Input(num_labels)})\n",
    "    \n",
    "    epoch_size = 100000\n",
    "    minibatch_size = 200\n",
    "    \n",
    "    # Define learning rate schedule\n",
    "    #lr_per_sample = [0.01]*30 + [0.008]*30 + [0.006]*30 + [0.002]*30 + [0.0008]\n",
    "    #lr_per_minibatch = [x * minibatch_size for x in lr_per_sample]\n",
    "    #lr_schedule = learning_rate_schedule([(15, 0.1), (15, 0.01), (15, 0.001), (1, 0.0001)], UnitType.sample, epoch_size)\n",
    "    lr_schedule = learning_rate_schedule(0.001, UnitType.sample)\n",
    "    \n",
    "    # Define momentum\n",
    "    #momentum_as_time_constant = momentum_as_time_constant_schedule(700)\n",
    "    m_schedule = momentum_schedule(0.95)\n",
    "    \n",
    "    # Define optimizer\n",
    "    #learner = adam_sgd(criterion.parameters, lr=lr_schedule, momentum=momentum_as_time_constant, \n",
    "    #                   low_memory=True, gradient_clipping_threshold_per_sample=15, gradient_clipping_with_truncation=True)\n",
    "    #learner = sgd(criterion.parameters, lr=lr_schedule)\n",
    "    learner = adam_sgd(criterion.parameters, lr=lr_schedule, momentum=m_schedule)\n",
    "    \n",
    "    # Define trainer\n",
    "    trainer = Trainer(model, criterion.outputs[0], criterion.outputs[1], learner)\n",
    "    \n",
    "    # Process minibatches and perform training\n",
    "    log_number_of_parameters(model)\n",
    "    progress_printer = ProgressPrinter(freq=1000, tag='Training')\n",
    "    \n",
    "    t = 0\n",
    "    for epoch in range(max_epochs):\n",
    "        epoch_end = (epoch+1) * epoch_size\n",
    "        while t < epoch_end:\n",
    "            data = reader.next_minibatch(minibatch_size, input_map={\n",
    "                    criterion.arguments[0]: reader.streams.ic, \n",
    "                    criterion.arguments[1]: reader.streams.oc\n",
    "                })\n",
    "            trainer.train_minibatch(data)\n",
    "            t += data[criterion.arguments[1]].num_samples\n",
    "            progress_printer.update_with_trainer(trainer, with_metric=True)\n",
    "        loss, metric, actual_samples = progress_printer.epoch_summary(with_metric=True)\n",
    "    \n",
    "    return loss, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 325168 parameters in 7 parameter tensors.\n",
      "Finished Epoch [1]: [Training] loss = 2.589912 * 100138, metric = 75.6% * 100138 1.216s (82320.1 samples per second)\n",
      "Finished Epoch [2]: [Training] loss = 2.468011 * 99938, metric = 74.4% * 99938 1.163s (85950.1 samples per second)\n",
      "Finished Epoch [3]: [Training] loss = 2.460211 * 99938, metric = 74.4% * 99938 1.155s (86545.7 samples per second)\n",
      "Finished Epoch [4]: [Training] loss = 2.457896 * 100184, metric = 74.2% * 100184 1.167s (85823.7 samples per second)\n",
      "Finished Epoch [5]: [Training] loss = 2.456179 * 99938, metric = 74.2% * 99938 1.150s (86879.7 samples per second)\n",
      "Finished Epoch [6]: [Training] loss = 2.451255 * 99938, metric = 74.1% * 99938 1.155s (86535.2 samples per second)\n",
      "Finished Epoch [7]: [Training] loss = 2.454772 * 99938, metric = 74.3% * 99938 1.163s (85962.3 samples per second)\n",
      "Finished Epoch [8]: [Training] loss = 2.450090 * 100184, metric = 74.1% * 100184 1.155s (86768.2 samples per second)\n",
      "Finished Epoch [9]: [Training] loss = 2.448356 * 99938, metric = 74.2% * 99938 1.147s (87092.7 samples per second)\n",
      "Finished Epoch [10]: [Training] loss = 2.446285 * 99938, metric = 74.2% * 99938 1.158s (86271.3 samples per second)\n",
      "Finished Epoch [11]: [Training] loss = 2.448923 * 99938, metric = 74.2% * 99938 1.176s (84996.9 samples per second)\n",
      "Finished Epoch [12]: [Training] loss = 2.448759 * 100184, metric = 74.1% * 100184 1.139s (87978.7 samples per second)\n",
      "Finished Epoch [13]: [Training] loss = 2.445445 * 99938, metric = 74.1% * 99938 1.163s (85906.0 samples per second)\n",
      "Finished Epoch [14]: [Training] loss = 2.446418 * 99938, metric = 74.0% * 99938 1.153s (86652.1 samples per second)\n",
      "Finished Epoch [15]: [Training] loss = 2.445511 * 99938, metric = 74.1% * 99938 1.186s (84292.0 samples per second)\n",
      "Finished Epoch [16]: [Training] loss = 2.443657 * 100184, metric = 74.0% * 100184 1.184s (84621.4 samples per second)\n",
      "Finished Epoch [17]: [Training] loss = 2.444660 * 99938, metric = 74.1% * 99938 1.185s (84339.1 samples per second)\n",
      "Finished Epoch [18]: [Training] loss = 2.441107 * 99938, metric = 74.0% * 99938 1.179s (84741.6 samples per second)\n",
      "Finished Epoch [19]: [Training] loss = 2.444424 * 99938, metric = 74.1% * 99938 1.169s (85461.4 samples per second)\n",
      "Finished Epoch [20]: [Training] loss = 2.440404 * 100184, metric = 74.1% * 100184 1.185s (84539.1 samples per second)\n",
      "Finished Epoch [21]: [Training] loss = 2.443817 * 99938, metric = 74.0% * 99938 1.226s (81493.4 samples per second)\n",
      "Finished Epoch [22]: [Training] loss = 2.438223 * 99938, metric = 74.0% * 99938 1.297s (77063.4 samples per second)\n",
      "Finished Epoch [23]: [Training] loss = 2.440677 * 99984, metric = 74.0% * 99984 1.386s (72143.8 samples per second)\n",
      "Finished Epoch [24]: [Training] loss = 2.438126 * 100138, metric = 73.9% * 100138 1.364s (73412.8 samples per second)\n",
      "Finished Epoch [25]: [Training] loss = 2.439330 * 99938, metric = 74.1% * 99938 1.467s (68130.7 samples per second)\n",
      "Finished Epoch [26]: [Training] loss = 2.435851 * 99938, metric = 73.9% * 99938 1.544s (64716.2 samples per second)\n",
      "Finished Epoch [27]: [Training] loss = 2.437309 * 99984, metric = 73.9% * 99984 1.720s (58115.1 samples per second)\n",
      "Finished Epoch [28]: [Training] loss = 2.437722 * 100138, metric = 74.1% * 100138 1.415s (70789.1 samples per second)\n",
      "Finished Epoch [29]: [Training] loss = 2.437299 * 99938, metric = 73.8% * 99938 1.709s (58475.9 samples per second)\n",
      "Finished Epoch [30]: [Training] loss = 2.438309 * 99938, metric = 73.9% * 99938 1.382s (72340.1 samples per second)\n",
      "Finished Epoch [31]: [Training] loss = 2.436351 * 99984, metric = 73.9% * 99984 1.335s (74868.1 samples per second)\n",
      "Finished Epoch [32]: [Training] loss = 2.438421 * 100138, metric = 74.1% * 100138 1.399s (71562.2 samples per second)\n",
      "Finished Epoch [33]: [Training] loss = 2.435587 * 99938, metric = 73.9% * 99938 1.432s (69799.7 samples per second)\n",
      "Finished Epoch [34]: [Training] loss = 2.434818 * 99938, metric = 73.9% * 99938 1.319s (75775.9 samples per second)\n",
      "Finished Epoch [35]: [Training] loss = 2.436217 * 99984, metric = 73.8% * 99984 1.535s (65142.4 samples per second)\n",
      "Finished Epoch [36]: [Training] loss = 2.434845 * 100138, metric = 73.9% * 100138 1.348s (74282.7 samples per second)\n",
      "Finished Epoch [37]: [Training] loss = 2.433494 * 99938, metric = 73.9% * 99938 1.354s (73824.1 samples per second)\n",
      "Finished Epoch [38]: [Training] loss = 2.437096 * 99938, metric = 73.9% * 99938 1.445s (69168.7 samples per second)\n",
      "Finished Epoch [39]: [Training] loss = 2.433930 * 99984, metric = 73.8% * 99984 1.248s (80137.6 samples per second)\n",
      "Finished Epoch [40]: [Training] loss = 2.434545 * 100138, metric = 73.9% * 100138 1.291s (77551.1 samples per second)\n",
      "Finished Epoch [41]: [Training] loss = 2.434052 * 99938, metric = 74.0% * 99938 1.351s (73946.8 samples per second)\n",
      "Finished Epoch [42]: [Training] loss = 2.434932 * 99984, metric = 74.0% * 99984 1.310s (76295.7 samples per second)\n",
      "Finished Epoch [43]: [Training] loss = 2.433026 * 99938, metric = 73.8% * 99938 1.236s (80837.2 samples per second)\n",
      "Finished Epoch [44]: [Training] loss = 2.431657 * 100138, metric = 73.9% * 100138 1.261s (79404.5 samples per second)\n",
      "Finished Epoch [45]: [Training] loss = 2.433152 * 99938, metric = 73.9% * 99938 1.257s (79495.1 samples per second)\n",
      "Finished Epoch [46]: [Training] loss = 2.431096 * 99984, metric = 73.8% * 99984 1.318s (75868.8 samples per second)\n",
      "Finished Epoch [47]: [Training] loss = 2.434339 * 99938, metric = 73.9% * 99938 1.353s (73874.4 samples per second)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "<cntk.cntk_py.StreamInformation; proxy of <Swig Object of type 'CNTK::StreamInformation *' at 0x0000000007466DB0> >",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f98478cd58fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdo_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-f98478cd58fa>\u001b[0m in \u001b[0;36mdo_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdo_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-1b73cf774849>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(reader, model, max_epochs)\u001b[0m\n\u001b[1;32m     60\u001b[0m             data = reader.next_minibatch(minibatch_size, input_map={\n\u001b[1;32m     61\u001b[0m                     \u001b[0mcriterion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstreams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mic\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                     \u001b[0mcriterion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstreams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 })\n\u001b[1;32m     64\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\cntk-py34\\lib\\site-packages\\cntk\\utils\\swig_helper.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mmap_if_possible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\cntk-py34\\lib\\site-packages\\cntk\\io\\__init__.py\u001b[0m in \u001b[0;36mnext_minibatch\u001b[0;34m(self, minibatch_size_in_samples, minibatch_size_in_sequences, input_map, device)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mmb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\cntk-py34\\lib\\site-packages\\cntk\\io\\__init__.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mmb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: <cntk.cntk_py.StreamInformation; proxy of <Swig Object of type 'CNTK::StreamInformation *' at 0x0000000007466DB0> >"
     ]
    }
   ],
   "source": [
    "def do_train():\n",
    "    global model\n",
    "    model = create_model()\n",
    "    reader = create_reader(data_path, is_training=True)\n",
    "    train(reader, model)\n",
    "do_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "reader = create_reader(data_path, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/sophie_elise_text.txt', 'r', encoding='utf8') as text:\n",
    "    source_text = text.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_text = 'abcdefghijklmnopqrstuvwxyz' * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_per_sample = [0.01]*30 + [0.008]*30 + [0.006]*30 + [0.002]*30 + [0.0008]\n",
    "\n",
    "minibatch_size = 14\n",
    "epoch_size = nb_sequences\n",
    "\n",
    "lr_per_minibatch = [x * minibatch_size for x in lr_per_sample]\n",
    "lr_schedule = learning_rate_schedule(lr_per_minibatch, epoch_size, UnitType.minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion = create_criterion_function(model)\n",
    "criterion.replace_placeholders({criterion.placeholders[0]: Input(vocab_size), \n",
    "                                criterion.placeholders[1]: Input(num_labels)})\n",
    "\n",
    "epoch_size = nb_sequences\n",
    "minibatch_size = 1\n",
    "\n",
    "# Define learning rate schedule\n",
    "lr_per_sample = [0.1]*50 + [0.001]*50 + [0.0001]\n",
    "lr_per_minibatch = [x * minibatch_size for x in lr_per_sample]\n",
    "lr_schedule = learning_rate_schedule(lr_per_minibatch, epoch_size, UnitType.minibatch)\n",
    "\n",
    "# Define momentum\n",
    "momentum_as_time_constant = momentum_as_time_constant_schedule(700)\n",
    "\n",
    "# Define optimizer\n",
    "#learner = adam_sgd(criterion.parameters, lr=lr_schedule, momentum=momentum_as_time_constant, \n",
    "#                   low_memory=True, gradient_clipping_threshold_per_sample=15, gradient_clipping_with_truncation=True)\n",
    "#learner = sgd(criterion.parameters, lr=lr_schedule)\n",
    "learner = adam_sgd(criterion.parameters, lr=lr_schedule, momentum=0.9)\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(model, criterion.outputs[0], criterion.outputs[1], learner)\n",
    "\n",
    "# Process minibatches and perform training\n",
    "log_number_of_parameters(model)\n",
    "progress_printer = ProgressPrinter(freq=10000, first=10, tag='Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = reader.next_minibatch(minibatch_size, input_map={\n",
    "                    criterion.arguments[0]: reader.streams.ic, \n",
    "                    criterion.arguments[1]: reader.streams.oc\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(reader.next_minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion.arguments[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir(criterion.arguments[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
