{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# char-rnn.py\n",
    "# Neural Character Language Model in CNTK2\n",
    "# wdarling@microsoft.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from cntk import Trainer, Axis\n",
    "from cntk.learner import adam_sgd, momentum_sgd, momentum_as_time_constant_schedule, learning_rate_schedule, UnitType\n",
    "from cntk.ops import input_variable, cross_entropy_with_softmax, classification_error\n",
    "from cntk.persist import load_model, save_model\n",
    "from cntk.blocks import LSTM, Stabilizer\n",
    "from cntk.layers import Recurrence, Dense, Dropout, BatchNormalization\n",
    "from cntk.utils import get_train_eval_criterion, get_train_loss\n",
    "from cntk.device import set_default_device, gpu\n",
    "\n",
    "# Set to GPU, run if GPU is available\n",
    "#set_default_device(gpu(0))\n",
    "\n",
    "# model hyperparameters\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "minibatch_size = 100 # also how much time we unroll the RNN for\n",
    "\n",
    "# Get data\n",
    "def get_data(p, minibatch_size, data, char_to_ix, vocab_dim):\n",
    "\n",
    "    xi = [char_to_ix[ch] for ch in data[p:p+minibatch_size]]\n",
    "    yi = [char_to_ix[ch] for ch in data[p+1:p+minibatch_size+1]]\n",
    "    \n",
    "    X = np.eye(vocab_dim, dtype=np.float32)[xi]\n",
    "    Y = np.eye(vocab_dim, dtype=np.float32)[yi]\n",
    "\n",
    "    # return a list of numpy arrays for each of X (features) and Y (labels)\n",
    "    return [X], [Y]\n",
    "\n",
    "# Sample from the network\n",
    "def sample(root, ix_to_char, vocab_dim, char_to_ix, prime_text='', use_hardmax=False, length=300, temperature=1.0):\n",
    "\n",
    "    # temperature: T < 1 means smoother; T=1.0 means same; T > 1 means more peaked\n",
    "    def apply_temp(p):\n",
    "        # apply temperature\n",
    "        p = np.power(p, (temperature))\n",
    "        # renormalize and return\n",
    "        return (p / np.sum(p))\n",
    "\n",
    "    def sample_word(p):\n",
    "        if use_hardmax:\n",
    "            w = np.argmax(p, axis=2)[0,0]\n",
    "        else:\n",
    "            # normalize probabilities then take weighted sample\n",
    "            p = np.exp(p) / np.sum(np.exp(p))            \n",
    "            p = apply_temp(p)\n",
    "            w = np.random.choice(range(vocab_dim), p=p.ravel())\n",
    "        return w\n",
    "\n",
    "    plen = 1\n",
    "    prime = -1\n",
    "\n",
    "    # start sequence with first input    \n",
    "    x = np.zeros((1, vocab_dim), dtype=np.float32)    \n",
    "    if prime_text != '':\n",
    "        plen = len(prime_text)\n",
    "        prime = char_to_ix[prime_text[0]]\n",
    "    else:\n",
    "        prime = np.random.choice(range(vocab_dim))\n",
    "    x[0, prime] = 1\n",
    "    arguments = ([x], [True])\n",
    "\n",
    "    output=[]\n",
    "    output.append(prime)\n",
    "    \n",
    "    # loop through prime text\n",
    "    for i in range(plen):            \n",
    "        p = root.eval(arguments)        \n",
    "        \n",
    "        # reset\n",
    "        x = np.zeros((1, vocab_dim), dtype=np.float32)\n",
    "        if i < plen-1:\n",
    "            idx = char_to_ix[prime_text[i+1]]\n",
    "        else:\n",
    "            idx = sample_word(p)\n",
    "\n",
    "        output.append(idx)\n",
    "        x[0, idx] = 1            \n",
    "        arguments = ([x], [False])\n",
    "    \n",
    "    # loop through length of generated text, sampling along the way\n",
    "    for i in range(length-plen):\n",
    "        p = root.eval(arguments)\n",
    "        idx = sample_word(p)\n",
    "        output.append(idx)\n",
    "\n",
    "        x = np.zeros((1, vocab_dim), dtype=np.float32)\n",
    "        x[0, idx] = 1\n",
    "        arguments = ([x], [False])\n",
    "\n",
    "    # return output\n",
    "    return ''.join([ix_to_char[c] for c in output])\n",
    "\n",
    "def load_data_and_vocab(training_file, convert_to_lower=True):\n",
    "    \n",
    "    # load data\n",
    "    rel_path = training_file\n",
    "    path = rel_path\n",
    "    data = open(path, \"r\", encoding='utf8').read()\n",
    "    \n",
    "    # Do some simple text prep\n",
    "    if convert_to_lower == True:\n",
    "        data = data.lower()\n",
    "    replacements = [[\"’\", \"'\"], \n",
    "                    ['“', '\"'], \n",
    "                    ['”', '\"'], \n",
    "                    [\"`\", \"'\"], \n",
    "                    ['[', '('], \n",
    "                    [']', ')']]\n",
    "    for r in replacements:\n",
    "        data = data.replace(r[0], r[1])\n",
    "        \n",
    "    chars = sorted(list(set(data)))\n",
    "    data_size, vocab_size = len(data), len(chars)\n",
    "    print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "    char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "    ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "    # write vocab\n",
    "    ff = open(path + \".vocab\", \"w\", encoding='utf8')\n",
    "    for c in chars:\n",
    "        ff.write(\"%s\\n\" % c) if c != '\\n' else ff.write(\"\\n\")\n",
    "    ff.close()\n",
    "    \n",
    "    return data, char_to_ix, ix_to_char, data_size, vocab_size\n",
    "\n",
    "# Creates and trains a character-level language model\n",
    "def train_lm(training_file, model_path, nb_epochs=1):\n",
    "\n",
    "    # create the stabilizer function from blocks\n",
    "    stabilize = Stabilizer()\n",
    "\n",
    "    # load the data and vocab\n",
    "    data, char_to_ix, ix_to_char, data_size, vocab_dim = load_data_and_vocab(training_file)\n",
    "\n",
    "    # Source and target inputs to the model\n",
    "    batch_axis = Axis.default_batch_axis()\n",
    "    input_seq_axis = Axis('inputAxis')\n",
    "\n",
    "    input_dynamic_axes = [batch_axis, input_seq_axis]\n",
    "    raw_input = input_variable(shape=(vocab_dim), dynamic_axes=input_dynamic_axes)\n",
    "    raw_labels = input_variable(shape=(vocab_dim), dynamic_axes=input_dynamic_axes)\n",
    "\n",
    "    input_sequence = raw_input\n",
    "    label_sequence = raw_labels\n",
    "\n",
    "    # LSTM\n",
    "    encoder_output = stabilize(input_sequence)\n",
    "    for i in range(0, num_layers):\n",
    "        encoder_output = Recurrence(LSTM(hidden_dim, enable_self_stabilization=True)) (encoder_output.output)\n",
    "        encoder_output = Dropout(0.5) (encoder_output.output)\n",
    "\n",
    "    # get output of the LSTM\n",
    "    states = encoder_output.output\n",
    "\n",
    "    # dense layer    \n",
    "    z = Dense(vocab_dim) (states)\n",
    "    print(z)\n",
    "\n",
    "    ce = cross_entropy_with_softmax(z, label_sequence)\n",
    "    errs = classification_error(z, label_sequence)\n",
    "\n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    lr_per_sample = learning_rate_schedule(0.001, UnitType.sample)\n",
    "    momentum_time_constant = momentum_as_time_constant_schedule(1100)\n",
    "    clipping_threshold_per_sample = 5.0\n",
    "    gradient_clipping_with_truncation = True\n",
    "    learner = adam_sgd(z.parameters, lr_per_sample, momentum_time_constant, \n",
    "                           gradient_clipping_threshold_per_sample=clipping_threshold_per_sample,\n",
    "                           gradient_clipping_with_truncation=gradient_clipping_with_truncation)\n",
    "    trainer = Trainer(z, ce, errs, learner)\n",
    "\n",
    "    training_progress_output_freq = 100\n",
    "    sample_freq = 1000\n",
    "    epochs = nb_epochs\n",
    "    minibatches_per_epoch = int((data_size / minibatch_size))\n",
    "    minibatches = epochs * minibatches_per_epoch\n",
    "    \n",
    "    e = 0\n",
    "    p = 0\n",
    "    for i in range(0, minibatches):\n",
    "\n",
    "        if p + minibatch_size+1 >= data_size:\n",
    "            p = 0\n",
    "            e += 1\n",
    "            model_filename = model_path % e\n",
    "            save_model(z, model_filename)\n",
    "            print(\"Saved model to '%s'\" % model_filename)\n",
    "\n",
    "        # get the data            \n",
    "        features, labels = get_data(p, minibatch_size, data, char_to_ix, vocab_dim)\n",
    "\n",
    "        # Specify the mapping of input variables in the model to actual minibatch data to be trained with\n",
    "        # If it's the start of the data, we specify that we are looking at a new sequence (True)\n",
    "        mask = [False] \n",
    "        if p == 0:\n",
    "            mask = [True]\n",
    "        arguments = ({raw_input : features, raw_labels : labels}, mask)\n",
    "        trainer.train_minibatch(arguments)\n",
    "\n",
    "        if i % training_progress_output_freq == 0:\n",
    "            print(\"Minibatch: {}, Train Loss: {}, Train Evaluation Criterion: {}\".format(i,\n",
    "                      get_train_loss(trainer), get_train_eval_criterion(trainer)))\n",
    "            print(\"Epoch %d, %f %% done\" % (e, ((float(i) / float(minibatches_per_epoch)) - e) * 100.0))\n",
    "        \n",
    "        if i % sample_freq == 0:\n",
    "            print(sample(z, ix_to_char, vocab_dim, char_to_ix))\n",
    "\n",
    "        p += minibatch_size\n",
    "\n",
    "# Creates and trains a character-level language model\n",
    "def train_multitask_lm(training_file, training_file_second, model_path, model_path_second, \n",
    "                       nb_epochs=1, nb_epochs_second=1, alternate=False):\n",
    "\n",
    "    # create the stabilizer function from blocks\n",
    "    stabilize = Stabilizer()\n",
    "\n",
    "    # load the data and vocab\n",
    "    data, char_to_ix, ix_to_char, data_size, vocab_dim = load_data_and_vocab(training_file)\n",
    "\n",
    "    # Source and target inputs to the model\n",
    "    batch_axis = Axis.default_batch_axis()\n",
    "    input_seq_axis = Axis('inputAxis')\n",
    "\n",
    "    input_dynamic_axes = [batch_axis, input_seq_axis]\n",
    "    raw_input = input_variable(shape=(vocab_dim), dynamic_axes=input_dynamic_axes)\n",
    "    raw_labels = input_variable(shape=(vocab_dim), dynamic_axes=input_dynamic_axes)\n",
    "\n",
    "    input_sequence = raw_input\n",
    "    label_sequence = raw_labels\n",
    "\n",
    "    # LSTM\n",
    "    encoder_output = stabilize(input_sequence)\n",
    "    for i in range(0, num_layers):\n",
    "        encoder_output = Recurrence(LSTM(hidden_dim, enable_self_stabilization=True)) (encoder_output.output)\n",
    "        encoder_output = Dropout(0.5) (encoder_output.output)\n",
    "\n",
    "    # get output of the LSTM\n",
    "    states = encoder_output.output\n",
    "\n",
    "    # dense layer    \n",
    "    z = Dense(vocab_dim) (states)\n",
    "\n",
    "    ce = cross_entropy_with_softmax(z, label_sequence)\n",
    "    errs = classification_error(z, label_sequence)\n",
    "\n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    lr_per_sample = learning_rate_schedule(0.001, UnitType.sample)\n",
    "    momentum_time_constant = momentum_as_time_constant_schedule(1100)\n",
    "    clipping_threshold_per_sample = 5.0\n",
    "    gradient_clipping_with_truncation = True\n",
    "    learner = adam_sgd(z.parameters, lr_per_sample, momentum_time_constant, \n",
    "                           gradient_clipping_threshold_per_sample=clipping_threshold_per_sample,\n",
    "                           gradient_clipping_with_truncation=gradient_clipping_with_truncation)\n",
    "    trainer = Trainer(z, ce, errs, learner)\n",
    "\n",
    "    training_progress_output_freq = 100\n",
    "    sample_freq = 1000\n",
    "    \n",
    "    if alternate == False:\n",
    "\n",
    "        epochs = nb_epochs\n",
    "        minibatches_per_epoch = int((data_size / minibatch_size))\n",
    "        minibatches = epochs * minibatches_per_epoch\n",
    "\n",
    "        e = 0\n",
    "        p = 0\n",
    "        for i in range(0, minibatches):\n",
    "\n",
    "            if p + minibatch_size+1 >= data_size:\n",
    "                p = 0\n",
    "                e += 1\n",
    "                model_filename = model_path % e\n",
    "                save_model(z, model_filename)\n",
    "                print(\"Saved model to '%s'\" % model_filename)\n",
    "\n",
    "            # get the data            \n",
    "            features, labels = get_data(p, minibatch_size, data, char_to_ix, vocab_dim)\n",
    "\n",
    "            # Specify the mapping of input variables in the model to actual minibatch data to be trained with\n",
    "            # If it's the start of the data, we specify that we are looking at a new sequence (True)\n",
    "            mask = [False] \n",
    "            if p == 0:\n",
    "                mask = [True]\n",
    "            arguments = ({raw_input : features, raw_labels : labels}, mask)\n",
    "            trainer.train_minibatch(arguments)\n",
    "\n",
    "            if i % training_progress_output_freq == 0:\n",
    "                print(\"Minibatch: {}, Train Loss: {}, Train Evaluation Criterion: {}\".format(i,\n",
    "                          get_train_loss(trainer), get_train_eval_criterion(trainer)))\n",
    "                print(\"Epoch %d, %f %% done\" % (e, ((float(i) / float(minibatches_per_epoch)) - e) * 100.0))\n",
    "\n",
    "            if i % sample_freq == 0:\n",
    "                print(sample(z, ix_to_char, vocab_dim, char_to_ix, prime_text='§'))\n",
    "\n",
    "            p += minibatch_size\n",
    "\n",
    "        # load the data and vocab\n",
    "        data, char_to_ix_, ix_to_char_, data_size, vocab_dim_ = load_data_and_vocab(training_file_second)\n",
    "\n",
    "        epochs = nb_epochs_second\n",
    "        minibatches_per_epoch = int((data_size / minibatch_size))\n",
    "        minibatches = epochs * minibatches_per_epoch\n",
    "\n",
    "        e = 0\n",
    "        p = 0\n",
    "        for i in range(0, minibatches):\n",
    "\n",
    "            if p + minibatch_size+1 >= data_size:\n",
    "                p = 0\n",
    "                e += 1\n",
    "                model_filename = model_path_second % e\n",
    "                save_model(z, model_filename)\n",
    "                print(\"Saved model to '%s'\" % model_filename)\n",
    "\n",
    "            # get the data            \n",
    "            features, labels = get_data(p, minibatch_size, data, char_to_ix, vocab_dim)\n",
    "\n",
    "            # Specify the mapping of input variables in the model to actual minibatch data to be trained with\n",
    "            # If it's the start of the data, we specify that we are looking at a new sequence (True)\n",
    "            mask = [False] \n",
    "            if p == 0:\n",
    "                mask = [True]\n",
    "            arguments = ({raw_input : features, raw_labels : labels}, mask)\n",
    "            trainer.train_minibatch(arguments)\n",
    "\n",
    "            if i % training_progress_output_freq == 0:\n",
    "                print(\"Minibatch: {}, Train Loss: {}, Train Evaluation Criterion: {}\".format(i,\n",
    "                          get_train_loss(trainer), get_train_eval_criterion(trainer)))\n",
    "                print(\"Epoch %d, %f %% done\" % (e, ((float(i) / float(minibatches_per_epoch)) - e) * 100.0))\n",
    "\n",
    "            if i % sample_freq == 0:\n",
    "                print(sample(z, ix_to_char, vocab_dim, char_to_ix, prime_text='|'))\n",
    "\n",
    "            p += minibatch_size\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        epochs = nb_epochs + nb_epochs_second\n",
    "        \n",
    "        for i in range(0, epochs):\n",
    "            \n",
    "            if i % 2 == 0:\n",
    "                data, char_to_ix, ix_to_char, data_size, vocab_dim = load_data_and_vocab(training_file)\n",
    "            else:\n",
    "                data, char_to_ix_, ix_to_char_, data_size, vocab_dim_ = load_data_and_vocab(training_file_second)\n",
    "            \n",
    "            minibatches_per_epoch = int((data_size / minibatch_size))\n",
    "            \n",
    "            e = 0\n",
    "            p = 0\n",
    "            for i in range(0, minibatches_per_epoch):\n",
    "\n",
    "                if p + minibatch_size+1 >= data_size:\n",
    "                    p = 0\n",
    "                    e += 1\n",
    "                    model_filename = model_path % e\n",
    "                    save_model(z, model_filename)\n",
    "                    print(\"Saved model to '%s'\" % model_filename)\n",
    "\n",
    "                # get the data            \n",
    "                features, labels = get_data(p, minibatch_size, data, char_to_ix, vocab_dim)\n",
    "\n",
    "                # Specify the mapping of input variables in the model to actual minibatch data to be trained with\n",
    "                # If it's the start of the data, we specify that we are looking at a new sequence (True)\n",
    "                mask = [False] \n",
    "                if p == 0:\n",
    "                    mask = [True]\n",
    "                arguments = ({raw_input : features, raw_labels : labels}, mask)\n",
    "                trainer.train_minibatch(arguments)\n",
    "\n",
    "                if i % training_progress_output_freq == 0:\n",
    "                    print(\"Minibatch: {}, Train Loss: {}, Train Evaluation Criterion: {}\".format(i,\n",
    "                              get_train_loss(trainer), get_train_eval_criterion(trainer)))\n",
    "                    print(\"Epoch %d, %f %% done\" % (e, ((float(i) / float(minibatches_per_epoch)) - e) * 100.0))\n",
    "\n",
    "                if i % sample_freq == 0:\n",
    "                    print(sample(z, ix_to_char, vocab_dim, char_to_ix, prime_text='§'))\n",
    "\n",
    "                p += minibatch_size\n",
    "        \n",
    "def load_and_sample(model_filename, vocab_filename, prime_text='', use_hardmax=False, length=1000, temperature=1.0):\n",
    "    \n",
    "    # load the model\n",
    "    model = load_model(model_filename)\n",
    "    \n",
    "    # load the vocab\n",
    "    chars = [c[0] for c in open(vocab_filename, encoding='utf8').readlines()]\n",
    "    char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "    ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "        \n",
    "    output = sample(model, ix_to_char, len(chars), char_to_ix, prime_text=prime_text, use_hardmax=use_hardmax, length=length, temperature=temperature)\n",
    "    \n",
    "    print(output)\n",
    "\n",
    "    #ff = open('output.txt', 'w', encoding='utf-8')\n",
    "    #ff.write(output)\n",
    "    #ff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1 % 2 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_multitask_lm(\"data/stories.txt\", \"data/songs.txt\", \n",
    "                   \"models/deepjingling-storyteller4_epoch%d.dnn\", \n",
    "                   \"models/deepjingling-songwriter4_epoch%d.dnn\", \n",
    "                   nb_epochs=10, nb_epochs_second=20, alternate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_lm(\"data/stories.txt\", \"models/deepjingling-storyteller4_epoch%d.dnn\", nb_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = \"|duck\"\n",
    "load_and_sample(\"models/deepjingling-songwriter4_epoch19.dnn\", \"data/songs.txt.vocab\", \n",
    "                prime_text=text, use_hardmax=False, length=200, temperature=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
