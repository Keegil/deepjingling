{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from cntk.blocks import default_options, LSTM, Placeholder, Input\n",
    "from cntk.layers import Embedding, Recurrence, Dense, BatchNormalization\n",
    "from cntk.models import Sequential\n",
    "from cntk.utils import ProgressPrinter, log_number_of_parameters\n",
    "from cntk.io import MinibatchSource, CTFDeserializer\n",
    "from cntk.io import StreamDef, StreamDefs\n",
    "from cntk.io import INFINITELY_REPEAT, FULL_DATA_SWEEP\n",
    "from cntk import *\n",
    "from cntk.learner import sgd, adam_sgd, learning_rate_schedule\n",
    "from cntk.device import set_default_device, gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_default_device(gpu(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "raw_data_path = 'data/songs.txt'\n",
    "data_path = 'data/songs_processed.ctf'\n",
    "dict_path = 'data/dict.ctf'\n",
    "\n",
    "with open(raw_data_path, encoding='utf8') as f:\n",
    "    source_text = f.read().lower()\n",
    "    \n",
    "replacements = [[\"’\", \"'\"], \n",
    "                ['“', '\"'], \n",
    "                ['”', '\"'],\n",
    "                ['\\n', '$']]\n",
    "\n",
    "# Make replacements\n",
    "for r in replacements:\n",
    "    source_text = source_text.replace(r[0], r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = [[k, v] for v, k in enumerate(sorted(set(source_text)))]\n",
    "char_dict = {key: value for (key, value) in chars}\n",
    "\n",
    "nb_songs = source_text.count('|')\n",
    "seq_max_length = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_text = ''\n",
    "nb_sequences = 0\n",
    "for n, char in enumerate(source_text[1:]):\n",
    "    #if n > 531:\n",
    "    #    break\n",
    "    prev_chars = source_text[max(0,(n+1-seq_max_length)):n+1]\n",
    "    if '|' in prev_chars:\n",
    "        prev_chars = prev_chars[max(0,prev_chars.index('|')):]\n",
    "    for k, prev_char in enumerate(prev_chars):\n",
    "        new_text += str(n) + '\\t|ic ' + str(char_dict[prev_char]) + ':1'\n",
    "        if k == 0:\n",
    "            new_text += '\\t|oc ' + str(char_dict[char]) + ':1'\n",
    "        new_text += '\\n'\n",
    "        nb_sequences += 1\n",
    "\n",
    "dict_text = ''\n",
    "for l in sorted(char_dict, key=char_dict.get):\n",
    "    dict_text += l + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(data_path, \"w\") as text_file:\n",
    "    text_file.write(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(dict_path, \"w\") as dict_file:\n",
    "    dict_file.write(dict_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_sequences = len(source_text) - 1\n",
    "#nb_sequences = 532\n",
    "nb_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of chars in vocabulary\n",
    "vocab_size = num_labels = len(char_dict)\n",
    "\n",
    "# Model dimensions\n",
    "input_dim = vocab_size\n",
    "label_dim = num_labels\n",
    "hidden_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to create model\n",
    "def create_model():\n",
    "    with default_options(initial_state=0.1):\n",
    "        return Sequential([\n",
    "                Recurrence(LSTM(hidden_dim), go_backwards=False), \n",
    "                BatchNormalization(),\n",
    "                Dense(num_labels)\n",
    "            ])\n",
    "\n",
    "def create_reader(path, is_training):\n",
    "    ic_stream = StreamDef(field='ic', shape=vocab_size, is_sparse=True)\n",
    "    oc_stream = StreamDef(field='oc', shape=num_labels, is_sparse=True)\n",
    "    stream_defs = StreamDefs(ic = ic_stream, oc = oc_stream)\n",
    "    ctf_deserializer = CTFDeserializer(path, stream_defs)\n",
    "    mbs = MinibatchSource(ctf_deserializer, randomize=is_training, \n",
    "                          epoch_size = INFINITELY_REPEAT if is_training else FULL_DATA_SWEEP)\n",
    "    return mbs\n",
    "\n",
    "def create_criterion_function(model):\n",
    "    labels = Placeholder()\n",
    "    ce = cross_entropy_with_softmax(model, labels)\n",
    "    errs = classification_error(model, labels)\n",
    "    return combine ([ce, errs])\n",
    "\n",
    "def train(reader, model, max_epochs=1000):\n",
    "    criterion = create_criterion_function(model)\n",
    "    criterion.replace_placeholders({criterion.placeholders[0]: Input(vocab_size), \n",
    "                                    criterion.placeholders[1]: Input(num_labels)})\n",
    "    \n",
    "    epoch_size = 100000\n",
    "    minibatch_size = 200\n",
    "    \n",
    "    # Define learning rate schedule\n",
    "    #lr_per_sample = [0.01]*30 + [0.008]*30 + [0.006]*30 + [0.002]*30 + [0.0008]\n",
    "    #lr_per_minibatch = [x * minibatch_size for x in lr_per_sample]\n",
    "    #lr_schedule = learning_rate_schedule([(15, 0.1), (15, 0.01), (15, 0.001), (1, 0.0001)], UnitType.sample, epoch_size)\n",
    "    lr_schedule = learning_rate_schedule(0.001, UnitType.sample)\n",
    "    \n",
    "    # Define momentum\n",
    "    #momentum_as_time_constant = momentum_as_time_constant_schedule(700)\n",
    "    m_schedule = momentum_schedule(0.95)\n",
    "    \n",
    "    # Define optimizer\n",
    "    #learner = adam_sgd(criterion.parameters, lr=lr_schedule, momentum=momentum_as_time_constant, \n",
    "    #                   low_memory=True, gradient_clipping_threshold_per_sample=15, gradient_clipping_with_truncation=True)\n",
    "    #learner = sgd(criterion.parameters, lr=lr_schedule)\n",
    "    learner = adam_sgd(criterion.parameters, lr=lr_schedule, momentum=m_schedule)\n",
    "    \n",
    "    # Define trainer\n",
    "    trainer = Trainer(model, criterion.outputs[0], criterion.outputs[1], learner)\n",
    "    \n",
    "    # Process minibatches and perform training\n",
    "    log_number_of_parameters(model)\n",
    "    progress_printer = ProgressPrinter(freq=1000, tag='Training')\n",
    "    \n",
    "    t = 0\n",
    "    for epoch in range(max_epochs):\n",
    "        epoch_end = (epoch+1) * epoch_size\n",
    "        while t < epoch_end:\n",
    "            data = reader.next_minibatch(minibatch_size, input_map={\n",
    "                    criterion.arguments[0]: reader.streams.ic, \n",
    "                    criterion.arguments[1]: reader.streams.oc\n",
    "                })\n",
    "            trainer.train_minibatch(data)\n",
    "            t += data[criterion.arguments[1]].num_samples\n",
    "            progress_printer.update_with_trainer(trainer, with_metric=True)\n",
    "        loss, metric, actual_samples = progress_printer.epoch_summary(with_metric=True)\n",
    "    \n",
    "    return loss, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def do_train():\n",
    "    global model\n",
    "    model = create_model()\n",
    "    reader = create_reader(data_path, is_training=True)\n",
    "    train(reader, model)\n",
    "do_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "reader = create_reader(data_path, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/sophie_elise_text.txt', 'r', encoding='utf8') as text:\n",
    "    source_text = text.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_text = 'abcdefghijklmnopqrstuvwxyz' * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_per_sample = [0.01]*30 + [0.008]*30 + [0.006]*30 + [0.002]*30 + [0.0008]\n",
    "\n",
    "minibatch_size = 14\n",
    "epoch_size = nb_sequences\n",
    "\n",
    "lr_per_minibatch = [x * minibatch_size for x in lr_per_sample]\n",
    "lr_schedule = learning_rate_schedule(lr_per_minibatch, epoch_size, UnitType.minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion = create_criterion_function(model)\n",
    "criterion.replace_placeholders({criterion.placeholders[0]: Input(vocab_size), \n",
    "                                criterion.placeholders[1]: Input(num_labels)})\n",
    "\n",
    "epoch_size = nb_sequences\n",
    "minibatch_size = 1\n",
    "\n",
    "# Define learning rate schedule\n",
    "lr_per_sample = [0.1]*50 + [0.001]*50 + [0.0001]\n",
    "lr_per_minibatch = [x * minibatch_size for x in lr_per_sample]\n",
    "lr_schedule = learning_rate_schedule(lr_per_minibatch, epoch_size, UnitType.minibatch)\n",
    "\n",
    "# Define momentum\n",
    "momentum_as_time_constant = momentum_as_time_constant_schedule(700)\n",
    "\n",
    "# Define optimizer\n",
    "#learner = adam_sgd(criterion.parameters, lr=lr_schedule, momentum=momentum_as_time_constant, \n",
    "#                   low_memory=True, gradient_clipping_threshold_per_sample=15, gradient_clipping_with_truncation=True)\n",
    "#learner = sgd(criterion.parameters, lr=lr_schedule)\n",
    "learner = adam_sgd(criterion.parameters, lr=lr_schedule, momentum=0.9)\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(model, criterion.outputs[0], criterion.outputs[1], learner)\n",
    "\n",
    "# Process minibatches and perform training\n",
    "log_number_of_parameters(model)\n",
    "progress_printer = ProgressPrinter(freq=10000, first=10, tag='Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = reader.next_minibatch(minibatch_size, input_map={\n",
    "                    criterion.arguments[0]: reader.streams.ic, \n",
    "                    criterion.arguments[1]: reader.streams.oc\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(reader.next_minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion.arguments[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir(criterion.arguments[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
