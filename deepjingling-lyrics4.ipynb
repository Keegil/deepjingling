{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# char-rnn.py\n",
    "# Neural Character Language Model in CNTK2\n",
    "# wdarling@microsoft.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 30647 characters, 48 unique.\n",
      "Minibatch: 0, Train Loss: 3.8713427734375, Train Evaluation Criterion: 0.97\n",
      "Epoch 0, 0.000000 % done\n",
      "flww w pppzpzqzpzpzpzzpzfpzpzpzpzzpzfpzpzpzpzzpzfpzpzpzpzzpzfpzpzpzpzzpzfpzpzpzpzzpzfpzpzpzpzzpzfpzpzpzpzzpzfpzpzpzpzzpzfpzpzpzpzzpzfpzpzpzpzzpzfpzpzpzpzzpzfpzpzpzpzzpzfpzpzpzpzzpzfpzpzpzpzzpzfpzpzpzpzzpzfpzpzpzpzzpzfpzpzpzpzzpzfpzpzpzpzzpzfpzpzpzpzzpzfpzpzpzpzzpzfpzpzpzpzzpzfpzpzpzpzzpzfpzpzpzpzzpzf\n",
      "Minibatch: 100, Train Loss: 3.041844482421875, Train Evaluation Criterion: 0.85\n",
      "Epoch 0, 32.679739 % done\n",
      "Minibatch: 200, Train Loss: 3.0899365234375, Train Evaluation Criterion: 0.87\n",
      "Epoch 0, 65.359477 % done\n",
      "Minibatch: 300, Train Loss: 2.9689752197265626, Train Evaluation Criterion: 0.8\n",
      "Epoch 0, 98.039216 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch1.dnn'\n",
      "Minibatch: 400, Train Loss: 2.9186004638671874, Train Evaluation Criterion: 0.8\n",
      "Epoch 1, 30.718954 % done\n",
      "Minibatch: 500, Train Loss: 2.509607696533203, Train Evaluation Criterion: 0.68\n",
      "Epoch 1, 63.398693 % done\n",
      "Minibatch: 600, Train Loss: 2.4167849731445314, Train Evaluation Criterion: 0.72\n",
      "Epoch 1, 96.078431 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch2.dnn'\n",
      "Minibatch: 700, Train Loss: 2.722874755859375, Train Evaluation Criterion: 0.77\n",
      "Epoch 2, 28.758170 % done\n",
      "Minibatch: 800, Train Loss: 2.1507833862304686, Train Evaluation Criterion: 0.64\n",
      "Epoch 2, 61.437908 % done\n",
      "Minibatch: 900, Train Loss: 2.089391632080078, Train Evaluation Criterion: 0.58\n",
      "Epoch 2, 94.117647 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch3.dnn'\n",
      "Minibatch: 1000, Train Loss: 2.0919985961914063, Train Evaluation Criterion: 0.62\n",
      "Epoch 3, 26.797386 % done\n",
      "0 a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a soulle a \n",
      "Minibatch: 1100, Train Loss: 2.5706915283203124, Train Evaluation Criterion: 0.63\n",
      "Epoch 3, 59.477124 % done\n",
      "Minibatch: 1200, Train Loss: 1.3077577209472657, Train Evaluation Criterion: 0.33\n",
      "Epoch 3, 92.156863 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch4.dnn'\n",
      "Minibatch: 1300, Train Loss: 2.159829406738281, Train Evaluation Criterion: 0.57\n",
      "Epoch 4, 24.836601 % done\n",
      "Minibatch: 1400, Train Loss: 1.883212890625, Train Evaluation Criterion: 0.48\n",
      "Epoch 4, 57.516340 % done\n",
      "Minibatch: 1500, Train Loss: 1.7958413696289062, Train Evaluation Criterion: 0.51\n",
      "Epoch 4, 90.196078 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch5.dnn'\n",
      "Minibatch: 1600, Train Loss: 1.5453140258789062, Train Evaluation Criterion: 0.44\n",
      "Epoch 5, 22.875817 % done\n",
      "Minibatch: 1700, Train Loss: 1.8930918884277343, Train Evaluation Criterion: 0.55\n",
      "Epoch 5, 55.555556 % done\n",
      "Minibatch: 1800, Train Loss: 1.4919398498535157, Train Evaluation Criterion: 0.43\n",
      "Epoch 5, 88.235294 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch6.dnn'\n",
      "Minibatch: 1900, Train Loss: 2.184544982910156, Train Evaluation Criterion: 0.57\n",
      "Epoch 6, 20.915033 % done\n",
      "Minibatch: 2000, Train Loss: 1.7738467407226564, Train Evaluation Criterion: 0.54\n",
      "Epoch 6, 53.594771 % done\n",
      "s and the will be and the will be and the will be and the will be and the will be and the will be and the will be and the will be and the will be and the will be and the will be and the will be and the will be and the will be and the will be and the will be and the will be and the will be and the wil\n",
      "Minibatch: 2100, Train Loss: 1.9062690734863281, Train Evaluation Criterion: 0.55\n",
      "Epoch 6, 86.274510 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch7.dnn'\n",
      "Minibatch: 2200, Train Loss: 1.97547119140625, Train Evaluation Criterion: 0.58\n",
      "Epoch 7, 18.954248 % done\n",
      "Minibatch: 2300, Train Loss: 1.749915313720703, Train Evaluation Criterion: 0.53\n",
      "Epoch 7, 51.633987 % done\n",
      "Minibatch: 2400, Train Loss: 1.8064572143554687, Train Evaluation Criterion: 0.5\n",
      "Epoch 7, 84.313725 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch8.dnn'\n",
      "Minibatch: 2500, Train Loss: 1.994735870361328, Train Evaluation Criterion: 0.6\n",
      "Epoch 8, 16.993464 % done\n",
      "Minibatch: 2600, Train Loss: 1.1513844299316407, Train Evaluation Criterion: 0.33\n",
      "Epoch 8, 49.673203 % done\n",
      "Minibatch: 2700, Train Loss: 1.7330661010742188, Train Evaluation Criterion: 0.55\n",
      "Epoch 8, 82.352941 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch9.dnn'\n",
      "Minibatch: 2800, Train Loss: 1.737886962890625, Train Evaluation Criterion: 0.52\n",
      "Epoch 9, 15.032680 % done\n",
      "Minibatch: 2900, Train Loss: 1.386607666015625, Train Evaluation Criterion: 0.39\n",
      "Epoch 9, 47.712418 % done\n",
      "Minibatch: 3000, Train Loss: 1.7024337768554687, Train Evaluation Criterion: 0.53\n",
      "Epoch 9, 80.392157 % done\n",
      "party the chimney tonight\n",
      "\n",
      "then a partring and night song of the chimney tonight\n",
      "\n",
      "then a partring and night song of the chimney tonight\n",
      "\n",
      "then a partring and night song of the chimney tonight\n",
      "\n",
      "then a partring and night song of the chimney tonight\n",
      "\n",
      "then a partring and night song of the chimney tonight\n",
      "\n",
      "Saved model to 'models/deepjingling-songwriter_epoch10.dnn'\n",
      "Minibatch: 3100, Train Loss: 2.100697326660156, Train Evaluation Criterion: 0.57\n",
      "Epoch 10, 13.071895 % done\n",
      "Minibatch: 3200, Train Loss: 1.519202880859375, Train Evaluation Criterion: 0.43\n",
      "Epoch 10, 45.751634 % done\n",
      "Minibatch: 3300, Train Loss: 1.3675894165039062, Train Evaluation Criterion: 0.43\n",
      "Epoch 10, 78.431373 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch11.dnn'\n",
      "Minibatch: 3400, Train Loss: 1.4705291748046876, Train Evaluation Criterion: 0.46\n",
      "Epoch 11, 11.111111 % done\n",
      "Minibatch: 3500, Train Loss: 1.892728271484375, Train Evaluation Criterion: 0.54\n",
      "Epoch 11, 43.790850 % done\n",
      "Minibatch: 3600, Train Loss: 1.9552383422851562, Train Evaluation Criterion: 0.52\n",
      "Epoch 11, 76.470588 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch12.dnn'\n",
      "Minibatch: 3700, Train Loss: 1.7583245849609375, Train Evaluation Criterion: 0.49\n",
      "Epoch 12, 9.150327 % done\n",
      "Minibatch: 3800, Train Loss: 1.5926504516601563, Train Evaluation Criterion: 0.44\n",
      "Epoch 12, 41.830065 % done\n",
      "Minibatch: 3900, Train Loss: 1.5478913879394531, Train Evaluation Criterion: 0.41\n",
      "Epoch 12, 74.509804 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch13.dnn'\n",
      "Minibatch: 4000, Train Loss: 1.0734296417236329, Train Evaluation Criterion: 0.32\n",
      "Epoch 13, 7.189542 % done\n",
      "here a laying\n",
      "5 golden rings\n",
      "4 calling in a winter wonderland\n",
      "\n",
      "when the snowman the snowman the snowman the snowman the snowman the snowman the snowman the snowman the snowman the snowman the snowman the snowman the snowman the snowman the snowman the snowman the snowman the snowman the snowman the s\n",
      "Minibatch: 4100, Train Loss: 1.5481056213378905, Train Evaluation Criterion: 0.42\n",
      "Epoch 13, 39.869281 % done\n",
      "Minibatch: 4200, Train Loss: 1.7439140319824218, Train Evaluation Criterion: 0.49\n",
      "Epoch 13, 72.549020 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch14.dnn'\n",
      "Minibatch: 4300, Train Loss: 1.2260977172851562, Train Evaluation Criterion: 0.35\n",
      "Epoch 14, 5.228758 % done\n",
      "Minibatch: 4400, Train Loss: 1.2400569915771484, Train Evaluation Criterion: 0.38\n",
      "Epoch 14, 37.908497 % done\n",
      "Minibatch: 4500, Train Loss: 1.1085848236083984, Train Evaluation Criterion: 0.35\n",
      "Epoch 14, 70.588235 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch15.dnn'\n",
      "Minibatch: 4600, Train Loss: 1.9943965148925782, Train Evaluation Criterion: 0.58\n",
      "Epoch 15, 3.267974 % done\n",
      "Minibatch: 4700, Train Loss: 1.3422323608398437, Train Evaluation Criterion: 0.37\n",
      "Epoch 15, 35.947712 % done\n",
      "Minibatch: 4800, Train Loss: 1.5736660766601562, Train Evaluation Criterion: 0.49\n",
      "Epoch 15, 68.627451 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch16.dnn'\n",
      "Minibatch: 4900, Train Loss: 1.6602725219726562, Train Evaluation Criterion: 0.49\n",
      "Epoch 16, 1.307190 % done\n",
      "Minibatch: 5000, Train Loss: 1.288118896484375, Train Evaluation Criterion: 0.38\n",
      "Epoch 16, 33.986928 % done\n",
      "ul believe\n",
      "\n",
      "your heart as santa claus, here comes santa claus, here comes santa claus, here comes santa claus, here comes santa claus, here comes santa claus, here comes santa claus, here comes santa claus, here comes santa claus, here comes santa claus, here comes santa claus, here comes santa claus\n",
      "Minibatch: 5100, Train Loss: 0.6262443161010742, Train Evaluation Criterion: 0.19\n",
      "Epoch 16, 66.666667 % done\n",
      "Minibatch: 5200, Train Loss: 1.649742889404297, Train Evaluation Criterion: 0.5\n",
      "Epoch 16, 99.346405 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch17.dnn'\n",
      "Minibatch: 5300, Train Loss: 1.0525086975097657, Train Evaluation Criterion: 0.33\n",
      "Epoch 17, 32.026144 % done\n",
      "Minibatch: 5400, Train Loss: 1.3193270874023437, Train Evaluation Criterion: 0.45\n",
      "Epoch 17, 64.705882 % done\n",
      "Minibatch: 5500, Train Loss: 1.0211317443847656, Train Evaluation Criterion: 0.34\n",
      "Epoch 17, 97.385621 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch18.dnn'\n",
      "Minibatch: 5600, Train Loss: 1.1827066802978516, Train Evaluation Criterion: 0.36\n",
      "Epoch 18, 30.065359 % done\n",
      "Minibatch: 5700, Train Loss: 0.7810188293457031, Train Evaluation Criterion: 0.22\n",
      "Epoch 18, 62.745098 % done\n",
      "Minibatch: 5800, Train Loss: 0.4092446517944336, Train Evaluation Criterion: 0.12\n",
      "Epoch 18, 95.424837 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch19.dnn'\n",
      "Minibatch: 5900, Train Loss: 1.300715789794922, Train Evaluation Criterion: 0.42\n",
      "Epoch 19, 28.104575 % done\n",
      "Minibatch: 6000, Train Loss: 1.1912894439697266, Train Evaluation Criterion: 0.37\n",
      "Epoch 19, 60.784314 % done\n",
      "the sounding to the snow\n",
      "\n",
      "there'll be home of christmas tree,\n",
      "there with the way\n",
      "he world to the snow\n",
      "\n",
      "there'll be home of christmas tree,\n",
      "there with the way\n",
      "he world to the snow\n",
      "\n",
      "there'll be home of christmas tree,\n",
      "there with the way\n",
      "he world to the snow\n",
      "\n",
      "there'll be home of christmas tree,\n",
      "there wi\n",
      "Minibatch: 6100, Train Loss: 0.3856045150756836, Train Evaluation Criterion: 0.11\n",
      "Epoch 19, 93.464052 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch20.dnn'\n",
      "Minibatch: 6200, Train Loss: 1.6427645874023438, Train Evaluation Criterion: 0.45\n",
      "Epoch 20, 26.143791 % done\n",
      "Minibatch: 6300, Train Loss: 1.0860162353515626, Train Evaluation Criterion: 0.35\n",
      "Epoch 20, 58.823529 % done\n",
      "Minibatch: 6400, Train Loss: 0.5445458602905273, Train Evaluation Criterion: 0.18\n",
      "Epoch 20, 91.503268 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch21.dnn'\n",
      "Minibatch: 6500, Train Loss: 1.6052670288085937, Train Evaluation Criterion: 0.5\n",
      "Epoch 21, 24.183007 % done\n",
      "Minibatch: 6600, Train Loss: 1.1529048156738282, Train Evaluation Criterion: 0.35\n",
      "Epoch 21, 56.862745 % done\n",
      "Minibatch: 6700, Train Loss: 0.48200057983398437, Train Evaluation Criterion: 0.14\n",
      "Epoch 21, 89.542484 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch22.dnn'\n",
      "Minibatch: 6800, Train Loss: 1.5340725708007812, Train Evaluation Criterion: 0.49\n",
      "Epoch 22, 22.222222 % done\n",
      "Minibatch: 6900, Train Loss: 1.080205535888672, Train Evaluation Criterion: 0.29\n",
      "Epoch 22, 54.901961 % done\n",
      "Minibatch: 7000, Train Loss: 0.4275703430175781, Train Evaluation Criterion: 0.12\n",
      "Epoch 22, 87.581699 % done\n",
      "and the snow\n",
      "\n",
      "there's a to the snow\n",
      "\n",
      "there's a to the snow\n",
      "\n",
      "there's a to the snow\n",
      "\n",
      "there's a to the snow\n",
      "\n",
      "there's a to the snow\n",
      "\n",
      "there's a to the snow\n",
      "\n",
      "there's a to the snow\n",
      "\n",
      "there's a to the snow\n",
      "\n",
      "there's a to the snow\n",
      "\n",
      "there's a to the snow\n",
      "\n",
      "there's a to the snow\n",
      "\n",
      "there's a to the snow\n",
      "\n",
      "there's a t\n",
      "Saved model to 'models/deepjingling-songwriter_epoch23.dnn'\n",
      "Minibatch: 7100, Train Loss: 1.5971337890625, Train Evaluation Criterion: 0.5\n",
      "Epoch 23, 20.261438 % done\n",
      "Minibatch: 7200, Train Loss: 1.2254827880859376, Train Evaluation Criterion: 0.37\n",
      "Epoch 23, 52.941176 % done\n",
      "Minibatch: 7300, Train Loss: 1.167079315185547, Train Evaluation Criterion: 0.36\n",
      "Epoch 23, 85.620915 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch24.dnn'\n",
      "Minibatch: 7400, Train Loss: 0.825589599609375, Train Evaluation Criterion: 0.23\n",
      "Epoch 24, 18.300654 % done\n",
      "Minibatch: 7500, Train Loss: 0.7846890258789062, Train Evaluation Criterion: 0.21\n",
      "Epoch 24, 50.980392 % done\n",
      "Minibatch: 7600, Train Loss: 1.503922882080078, Train Evaluation Criterion: 0.43\n",
      "Epoch 24, 83.660131 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch25.dnn'\n",
      "Minibatch: 7700, Train Loss: 0.9043115234375, Train Evaluation Criterion: 0.23\n",
      "Epoch 25, 16.339869 % done\n",
      "Minibatch: 7800, Train Loss: 0.8966200256347656, Train Evaluation Criterion: 0.28\n",
      "Epoch 25, 49.019608 % done\n",
      "Minibatch: 7900, Train Loss: 1.0604788208007812, Train Evaluation Criterion: 0.31\n",
      "Epoch 25, 81.699346 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch26.dnn'\n",
      "Minibatch: 8000, Train Loss: 1.2927749633789063, Train Evaluation Criterion: 0.4\n",
      "Epoch 26, 14.379085 % done\n",
      "ed the season to the sea\n",
      "with the season to the sea\n",
      "with the season to the sea\n",
      "with the season to the sea\n",
      "with the season to the sea\n",
      "with the season to the sea\n",
      "with the season to the sea\n",
      "with the season to the sea\n",
      "with the season to the sea\n",
      "with the season to the sea\n",
      "with the season to the sea\n",
      "with t\n",
      "Minibatch: 8100, Train Loss: 1.4522909545898437, Train Evaluation Criterion: 0.41\n",
      "Epoch 26, 47.058824 % done\n",
      "Minibatch: 8200, Train Loss: 1.123249740600586, Train Evaluation Criterion: 0.38\n",
      "Epoch 26, 79.738562 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch27.dnn'\n",
      "Minibatch: 8300, Train Loss: 1.3055853271484374, Train Evaluation Criterion: 0.4\n",
      "Epoch 27, 12.418301 % done\n",
      "Minibatch: 8400, Train Loss: 0.9557689666748047, Train Evaluation Criterion: 0.27\n",
      "Epoch 27, 45.098039 % done\n",
      "Minibatch: 8500, Train Loss: 0.9728251647949219, Train Evaluation Criterion: 0.33\n",
      "Epoch 27, 77.777778 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch28.dnn'\n",
      "Minibatch: 8600, Train Loss: 0.7841422271728515, Train Evaluation Criterion: 0.28\n",
      "Epoch 28, 10.457516 % done\n",
      "Minibatch: 8700, Train Loss: 1.3370567321777345, Train Evaluation Criterion: 0.41\n",
      "Epoch 28, 43.137255 % done\n",
      "Minibatch: 8800, Train Loss: 0.9210577392578125, Train Evaluation Criterion: 0.25\n",
      "Epoch 28, 75.816993 % done\n",
      "Saved model to 'models/deepjingling-songwriter_epoch29.dnn'\n",
      "Minibatch: 8900, Train Loss: 1.1349640655517579, Train Evaluation Criterion: 0.39\n",
      "Epoch 29, 8.496732 % done\n",
      "Minibatch: 9000, Train Loss: 1.523461151123047, Train Evaluation Criterion: 0.43\n",
      "Epoch 29, 41.176471 % done\n",
      "0 the snowman, was a come of street the star some sing,\n",
      "\"glory to the newborn king!!\n",
      "\n",
      "christ was born to star say there's not come in the snowman, was a come of street the star some sing,\n",
      "\"glory to the newborn king!!\n",
      "\n",
      "christ was born to star say there's not come in the snowman, was a come of street t\n",
      "Minibatch: 9100, Train Loss: 1.2666653442382811, Train Evaluation Criterion: 0.38\n",
      "Epoch 29, 73.856209 % done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from cntk import Trainer, Axis\n",
    "from cntk.learner import adam_sgd, momentum_sgd, momentum_as_time_constant_schedule, learning_rate_schedule, UnitType\n",
    "from cntk.ops import input_variable, cross_entropy_with_softmax, classification_error\n",
    "from cntk.persist import load_model, save_model\n",
    "from cntk.blocks import LSTM, Stabilizer\n",
    "from cntk.layers import Recurrence, Dense, Dropout, BatchNormalization\n",
    "from cntk.utils import get_train_eval_criterion, get_train_loss\n",
    "from cntk.device import set_default_device, gpu\n",
    "\n",
    "# Set to GPU, run if GPU is available\n",
    "#set_default_device(gpu(0))\n",
    "\n",
    "# model hyperparameters\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "minibatch_size = 100 # also how much time we unroll the RNN for\n",
    "\n",
    "# Get data\n",
    "def get_data(p, minibatch_size, data, char_to_ix, vocab_dim):\n",
    "\n",
    "    xi = [char_to_ix[ch] for ch in data[p:p+minibatch_size]]\n",
    "    yi = [char_to_ix[ch] for ch in data[p+1:p+minibatch_size+1]]\n",
    "    \n",
    "    X = np.eye(vocab_dim, dtype=np.float32)[xi]\n",
    "    Y = np.eye(vocab_dim, dtype=np.float32)[yi]\n",
    "\n",
    "    # return a list of numpy arrays for each of X (features) and Y (labels)\n",
    "    return [X], [Y]\n",
    "\n",
    "# Sample from the network\n",
    "def sample(root, ix_to_char, vocab_dim, char_to_ix, prime_text='', use_hardmax=True, length=300, temperature=1.0):\n",
    "\n",
    "    # temperature: T < 1 means smoother; T=1.0 means same; T > 1 means more peaked\n",
    "    def apply_temp(p):\n",
    "        # apply temperature\n",
    "        p = np.power(p, (temperature))\n",
    "        # renormalize and return\n",
    "        return (p / np.sum(p))\n",
    "\n",
    "    def sample_word(p):\n",
    "        if use_hardmax:\n",
    "            w = np.argmax(p, axis=2)[0,0]\n",
    "        else:\n",
    "            # normalize probabilities then take weighted sample\n",
    "            p = np.exp(p) / np.sum(np.exp(p))            \n",
    "            p = apply_temp(p)\n",
    "            w = np.random.choice(range(vocab_dim), p=p.ravel())\n",
    "        return w\n",
    "\n",
    "    plen = 1\n",
    "    prime = -1\n",
    "\n",
    "    # start sequence with first input    \n",
    "    x = np.zeros((1, vocab_dim), dtype=np.float32)    \n",
    "    if prime_text != '':\n",
    "        plen = len(prime_text)\n",
    "        prime = char_to_ix[prime_text[0]]\n",
    "    else:\n",
    "        prime = np.random.choice(range(vocab_dim))\n",
    "    x[0, prime] = 1\n",
    "    arguments = ([x], [True])\n",
    "\n",
    "    output=[]\n",
    "    output.append(prime)\n",
    "    \n",
    "    # loop through prime text\n",
    "    for i in range(plen):            \n",
    "        p = root.eval(arguments)        \n",
    "        \n",
    "        # reset\n",
    "        x = np.zeros((1, vocab_dim), dtype=np.float32)\n",
    "        if i < plen-1:\n",
    "            idx = char_to_ix[prime_text[i+1]]\n",
    "        else:\n",
    "            idx = sample_word(p)\n",
    "\n",
    "        output.append(idx)\n",
    "        x[0, idx] = 1            \n",
    "        arguments = ([x], [False])\n",
    "    \n",
    "    # loop through length of generated text, sampling along the way\n",
    "    for i in range(length-plen):\n",
    "        p = root.eval(arguments)\n",
    "        idx = sample_word(p)\n",
    "        output.append(idx)\n",
    "\n",
    "        x = np.zeros((1, vocab_dim), dtype=np.float32)\n",
    "        x[0, idx] = 1\n",
    "        arguments = ([x], [False])\n",
    "\n",
    "    # return output\n",
    "    return ''.join([ix_to_char[c] for c in output])\n",
    "\n",
    "def load_data_and_vocab(training_file, convert_to_lower=True):\n",
    "    \n",
    "    # load data\n",
    "    rel_path = training_file\n",
    "    path = rel_path\n",
    "    data = open(path, \"r\", encoding='utf8').read()\n",
    "    \n",
    "    # Do some simple text prep\n",
    "    if convert_to_lower == True:\n",
    "        data = data.lower()\n",
    "    replacements = [[\"’\", \"'\"], \n",
    "                    ['“', '\"'], \n",
    "                    ['”', '\"']]\n",
    "    for r in replacements:\n",
    "        data = data.replace(r[0], r[1])\n",
    "        \n",
    "    chars = sorted(list(set(data)))\n",
    "    data_size, vocab_size = len(data), len(chars)\n",
    "    print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "    char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "    ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "    # write vocab\n",
    "    ff = open(path + \".vocab\", \"w\", encoding='utf8')\n",
    "    for c in chars:\n",
    "        ff.write(\"%s\\n\" % c) if c != '\\n' else ff.write(\"\\n\")\n",
    "    ff.close()\n",
    "    \n",
    "    return data, char_to_ix, ix_to_char, data_size, vocab_size\n",
    "\n",
    "# Creates and trains a character-level language model\n",
    "def train_lm(training_file):\n",
    "\n",
    "    # create the stabilizer function from blocks\n",
    "    stabilize = Stabilizer()\n",
    "\n",
    "    # load the data and vocab\n",
    "    data, char_to_ix, ix_to_char, data_size, vocab_dim = load_data_and_vocab(training_file)\n",
    "\n",
    "    # Source and target inputs to the model\n",
    "    batch_axis = Axis.default_batch_axis()\n",
    "    input_seq_axis = Axis('inputAxis')\n",
    "\n",
    "    input_dynamic_axes = [batch_axis, input_seq_axis]\n",
    "    raw_input = input_variable(shape=(vocab_dim), dynamic_axes=input_dynamic_axes)\n",
    "    raw_labels = input_variable(shape=(vocab_dim), dynamic_axes=input_dynamic_axes)\n",
    "\n",
    "    input_sequence = raw_input\n",
    "    label_sequence = raw_labels\n",
    "\n",
    "    # LSTM\n",
    "    encoder_output = stabilize(input_sequence)\n",
    "    for i in range(0, num_layers):\n",
    "        encoder_output = Recurrence(LSTM(hidden_dim, enable_self_stabilization=True)) (encoder_output.output)\n",
    "        encoder_output = Dropout(0.5) (encoder_output.output)\n",
    "\n",
    "    # get output of the LSTM\n",
    "    states = encoder_output.output\n",
    "\n",
    "    # dense layer    \n",
    "    z = Dense(vocab_dim) (states)\n",
    "\n",
    "    ce = cross_entropy_with_softmax(z, label_sequence)\n",
    "    errs = classification_error(z, label_sequence)\n",
    "\n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    lr_per_sample = learning_rate_schedule(0.001, UnitType.sample)\n",
    "    momentum_time_constant = momentum_as_time_constant_schedule(1100)\n",
    "    clipping_threshold_per_sample = 5.0\n",
    "    gradient_clipping_with_truncation = True\n",
    "    learner = adam_sgd(z.parameters, lr_per_sample, momentum_time_constant, \n",
    "                           gradient_clipping_threshold_per_sample=clipping_threshold_per_sample,\n",
    "                           gradient_clipping_with_truncation=gradient_clipping_with_truncation)\n",
    "    trainer = Trainer(z, ce, errs, learner)\n",
    "\n",
    "    training_progress_output_freq = 100\n",
    "    sample_freq = 1000\n",
    "    epochs = 30\n",
    "    minibatches_per_epoch = int((data_size / minibatch_size))\n",
    "    minibatches = epochs * minibatches_per_epoch\n",
    "    \n",
    "    e = 0\n",
    "    p = 0\n",
    "    for i in range(0, minibatches):\n",
    "\n",
    "        if p + minibatch_size+1 >= data_size:\n",
    "            p = 0\n",
    "            e += 1\n",
    "            model_filename = \"models/deepjingling-songwriter_epoch%d.dnn\" % e\n",
    "            save_model(z, model_filename)\n",
    "            print(\"Saved model to '%s'\" % model_filename)\n",
    "\n",
    "        # get the data            \n",
    "        features, labels = get_data(p, minibatch_size, data, char_to_ix, vocab_dim)\n",
    "\n",
    "        # Specify the mapping of input variables in the model to actual minibatch data to be trained with\n",
    "        # If it's the start of the data, we specify that we are looking at a new sequence (True)\n",
    "        mask = [False] \n",
    "        if p == 0:\n",
    "            mask = [True]\n",
    "        arguments = ({raw_input : features, raw_labels : labels}, mask)\n",
    "        trainer.train_minibatch(arguments)\n",
    "\n",
    "        if i % training_progress_output_freq == 0:\n",
    "            print(\"Minibatch: {}, Train Loss: {}, Train Evaluation Criterion: {}\".format(i,\n",
    "                      get_train_loss(trainer), get_train_eval_criterion(trainer)))\n",
    "            print(\"Epoch %d, %f %% done\" % (e, ((float(i) / float(minibatches_per_epoch)) - e) * 100.0))\n",
    "        \n",
    "        if i % sample_freq == 0:\n",
    "            print(sample(z, ix_to_char, vocab_dim, char_to_ix))\n",
    "\n",
    "        p += minibatch_size\n",
    "\n",
    "def load_and_sample(model_filename, vocab_filename, prime_text='', use_hardmax=False, length=1000, temperature=1.0):\n",
    "    \n",
    "    # load the model\n",
    "    model = load_model(model_filename)\n",
    "    \n",
    "    # load the vocab\n",
    "    chars = [c[0] for c in open(vocab_filename).readlines()]\n",
    "    char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "    ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "        \n",
    "    #output = sample(model, ix_to_char, len(chars), char_to_ix, prime_text=prime_text, use_hardmax=use_hardmax, length=length, temperature=temperature)\n",
    "    \n",
    "    print(sample(model, ix_to_char, len(chars), char_to_ix))\n",
    "\n",
    "    #ff = open('output.txt', 'w', encoding='utf-8')\n",
    "    #ff.write(output)\n",
    "    #ff.close()\n",
    "\n",
    "if __name__=='__main__':    \n",
    "    \n",
    "    # train the LM\n",
    "    train_lm(\"data/songs.txt\")\n",
    "\n",
    "    # load and sample\n",
    "    #text = \"T\"\n",
    "    #load_and_sample(\"models/shakespeare_epoch13.dnn\", \"shakespeare.txt.vocab\", prime_text=text, use_hardmax=False, length=100, temperature=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s be merry and bright\n",
      "and a partridge in a pear tree\n",
      "\n",
      "on the snow \n",
      "there's a happy new year.\n",
      "\n",
      "we wish you a merry christmas\n",
      "my true love sent to me:\n",
      "11 diping\n",
      "10 lords a leaping\n",
      "7 swans a swimming\n",
      "6 geese a laying\n",
      "5 golden rings\n",
      "4 calling birds\n",
      "3 french hens\n",
      "2 turtle doves\n",
      "and a partridge in a pear \n"
     ]
    }
   ],
   "source": [
    "text = \"|\"\n",
    "load_and_sample(\"models/deepjingling-songwriter_epoch29.dnn\", \"data/songs.txt.vocab\", \n",
    "                prime_text=text, use_hardmax=False, length=200, temperature=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
